{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5742c7d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:11.216608Z",
     "iopub.status.busy": "2022-01-31T17:09:11.215591Z",
     "iopub.status.idle": "2022-01-31T17:09:11.223067Z",
     "shell.execute_reply": "2022-01-31T17:09:11.223694Z",
     "shell.execute_reply.started": "2022-01-31T15:09:25.58892Z"
    },
    "id": "f72cc994",
    "papermill": {
     "duration": 0.062608,
     "end_time": "2022-01-31T17:09:11.224035",
     "exception": false,
     "start_time": "2022-01-31T17:09:11.161427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "KAGGLE = True\n",
    "DEBUG  = False\n",
    "SAVE_SW = True\n",
    "YEAR_FILTER = True\n",
    "LEAKAGE_FILTER = True\n",
    "RETRAIN = False\n",
    "RESAMPLE = True\n",
    "ADV = True\n",
    "VALID = False\n",
    "SUPPLEMENT = True\n",
    "basesize = 2236494\n",
    "SUBTEST = False\n",
    "savepath = '../input/g-r-finaldataset'\n",
    "\n",
    "if KAGGLE:\n",
    "  save_directry = './'\n",
    "else:\n",
    "  save_directry = '/content/drive/MyDrive/Machine_Learning/data/kaggle/2021_gresearch_crypto/04_model/26_23codehosei'\n",
    "\n",
    "title_num = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9fa01b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:11.312763Z",
     "iopub.status.busy": "2022-01-31T17:09:11.311982Z",
     "iopub.status.idle": "2022-01-31T17:09:11.314336Z",
     "shell.execute_reply": "2022-01-31T17:09:11.314860Z",
     "shell.execute_reply.started": "2022-01-31T15:09:25.622713Z"
    },
    "papermill": {
     "duration": 0.047352,
     "end_time": "2022-01-31T17:09:11.315065",
     "exception": false,
     "start_time": "2022-01-31T17:09:11.267713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3601のところを高速化\n",
    "# 2019で実施\n",
    "# RETRAIN = True\n",
    "# SUBTESTをFalseに変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c719a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:11.400095Z",
     "iopub.status.busy": "2022-01-31T17:09:11.399314Z",
     "iopub.status.idle": "2022-01-31T17:09:11.405899Z",
     "shell.execute_reply": "2022-01-31T17:09:11.406497Z",
     "shell.execute_reply.started": "2022-01-31T15:09:25.627534Z"
    },
    "id": "914706f3",
    "outputId": "289d9c87-abd8-4b3e-e163-bdc1b9105595",
    "papermill": {
     "duration": 0.050398,
     "end_time": "2022-01-31T17:09:11.406714",
     "exception": false,
     "start_time": "2022-01-31T17:09:11.356316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3811f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:11.494739Z",
     "iopub.status.busy": "2022-01-31T17:09:11.493941Z",
     "iopub.status.idle": "2022-01-31T17:09:13.778794Z",
     "shell.execute_reply": "2022-01-31T17:09:13.779344Z",
     "shell.execute_reply.started": "2022-01-31T15:09:25.648946Z"
    },
    "id": "33ac9474",
    "papermill": {
     "duration": 2.329818,
     "end_time": "2022-01-31T17:09:13.779529",
     "exception": false,
     "start_time": "2022-01-31T17:09:11.449711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/g-research-46-maketrainfull-set2020start/2020_20210920.parquet\n",
      "/kaggle/input/g-research-46-maketrainfull-set2020start/26_dict_RM_macd.pkl\n",
      "/kaggle/input/g-research-46-maketrainfull-set2020start/__results__.html\n",
      "/kaggle/input/g-research-46-maketrainfull-set2020start/26_dict_RM_17.pkl\n",
      "/kaggle/input/g-research-46-maketrainfull-set2020start/26_dict_RM_lag.pkl\n",
      "/kaggle/input/g-research-46-maketrainfull-set2020start/26_dict_RM_market.pkl\n",
      "/kaggle/input/g-research-46-maketrainfull-set2020start/__notebook__.ipynb\n",
      "/kaggle/input/g-research-46-maketrainfull-set2020start/__output__.json\n",
      "/kaggle/input/g-research-46-maketrainfull-set2020start/26_dict_RM.pkl\n",
      "/kaggle/input/g-research-46-maketrainfull-set2020start/26_dict_RM_richbeta.pkl\n",
      "/kaggle/input/g-research-46-maketrainfull-set2020start/26_dict_RM_vols.pkl\n",
      "/kaggle/input/g-research-46-maketrainfull-set2020start/26_dict_RM_base.pkl\n",
      "/kaggle/input/g-research-46-maketrainfull-set2020start/custom.css\n",
      "/kaggle/input/28-lgbm/28_lgbm_models/lgbm__lightgbm.basic.Booster object at 0x7f5545bf6f90_.pkl\n",
      "/kaggle/input/28-lgbm/28_lgbm_models/lgbm__lightgbm.basic.Booster object at 0x7f5545bcc990_.pkl\n",
      "/kaggle/input/28-lgbm/28_lgbm_models/lgbm__lightgbm.basic.Booster object at 0x7f5545bcca10_.pkl\n",
      "/kaggle/input/28-lgbm/28_lgbm_models/lgbm__lightgbm.basic.Booster object at 0x7f5545bf6d50_.pkl\n",
      "/kaggle/input/28-lgbm/28_lgbm_models/lgbm__lightgbm.basic.Booster object at 0x7f5545bf6610_.pkl\n",
      "/kaggle/input/g-r-finaldataset/2020_20210920_d2.parquet\n",
      "/kaggle/input/g-r-finaldataset/26_dict_RM_macd.pkl\n",
      "/kaggle/input/g-r-finaldataset/26_dict_RM_17.pkl\n",
      "/kaggle/input/g-r-finaldataset/26_dict_RM_lag.pkl\n",
      "/kaggle/input/g-r-finaldataset/26_dict_RM_market.pkl\n",
      "/kaggle/input/g-r-finaldataset/26_dict_RM.pkl\n",
      "/kaggle/input/g-r-finaldataset/26_dict_RM_richbeta.pkl\n",
      "/kaggle/input/g-r-finaldataset/26_dict_RM_vols.pkl\n",
      "/kaggle/input/g-r-finaldataset/26_dict_RM_base.pkl\n",
      "/kaggle/input/g-research-crypto-forecasting/example_sample_submission.csv\n",
      "/kaggle/input/g-research-crypto-forecasting/asset_details.csv\n",
      "/kaggle/input/g-research-crypto-forecasting/example_test.csv\n",
      "/kaggle/input/g-research-crypto-forecasting/train.csv\n",
      "/kaggle/input/g-research-crypto-forecasting/supplemental_train.csv\n",
      "/kaggle/input/g-research-crypto-forecasting/gresearch_crypto/competition.cpython-37m-x86_64-linux-gnu.so\n",
      "/kaggle/input/g-research-crypto-forecasting/gresearch_crypto/__init__.py\n"
     ]
    }
   ],
   "source": [
    "if KAGGLE:\n",
    "  import gresearch_crypto\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import QuantileTransformer, StandardScaler\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import gc\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import psutil\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def timestamp_to_date(timestamp):\n",
    "    return(datetime.fromtimestamp(timestamp))\n",
    "\n",
    "\n",
    "if KAGGLE:\n",
    "  import os\n",
    "  for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "      for filename in filenames:\n",
    "          print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#env = gresearch_crypto.make_env()\n",
    "\n",
    "#iter_test = env.iter_test()\n",
    "\n",
    "#(test_df, sample_prediction_df) = next(iter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b90ca850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:13.875925Z",
     "iopub.status.busy": "2022-01-31T17:09:13.874846Z",
     "iopub.status.idle": "2022-01-31T17:09:13.877066Z",
     "shell.execute_reply": "2022-01-31T17:09:13.877663Z",
     "shell.execute_reply.started": "2022-01-31T15:09:27.842309Z"
    },
    "id": "eb137678",
    "outputId": "85d31f7a-40b6-4af9-8607-8f1d4648fb3c",
    "papermill": {
     "duration": 0.050435,
     "end_time": "2022-01-31T17:09:13.877832",
     "exception": false,
     "start_time": "2022-01-31T17:09:13.827397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "  pass\n",
    "else:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a087a257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:13.970371Z",
     "iopub.status.busy": "2022-01-31T17:09:13.969466Z",
     "iopub.status.idle": "2022-01-31T17:09:13.972205Z",
     "shell.execute_reply": "2022-01-31T17:09:13.971694Z",
     "shell.execute_reply.started": "2022-01-31T15:09:27.849071Z"
    },
    "papermill": {
     "duration": 0.05236,
     "end_time": "2022-01-31T17:09:13.972352",
     "exception": false,
     "start_time": "2022-01-31T17:09:13.919992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# peak memory\n",
    "import os\n",
    "import re\n",
    "\n",
    "def peak_memory():\n",
    "    pid = os.getpid()\n",
    "    with open(f'/proc/{pid}/status') as f:\n",
    "        for line in f:\n",
    "            if not line.startswith('VmHWM:'):\n",
    "                continue\n",
    "            return int(re.search('[0-9]+', line)[0])\n",
    "    raise ValueError('Not Found')\n",
    "\n",
    "def f():\n",
    "    a = [0] * 20000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea07230",
   "metadata": {
    "id": "11a3c93e",
    "papermill": {
     "duration": 0.042334,
     "end_time": "2022-01-31T17:09:14.056937",
     "exception": false,
     "start_time": "2022-01-31T17:09:14.014603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='Folds'></a>\n",
    "# Creating Training Folds\n",
    "\n",
    "For the design of the folds, see discussion here: https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/288555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95268642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:14.144136Z",
     "iopub.status.busy": "2022-01-31T17:09:14.143376Z",
     "iopub.status.idle": "2022-01-31T17:09:14.153156Z",
     "shell.execute_reply": "2022-01-31T17:09:14.153712Z",
     "shell.execute_reply.started": "2022-01-31T15:09:27.862421Z"
    },
    "id": "3hEBdDJVTwjE",
    "papermill": {
     "duration": 0.055306,
     "end_time": "2022-01-31T17:09:14.153911",
     "exception": false,
     "start_time": "2022-01-31T17:09:14.098605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if RETRAIN:\n",
    "    #nrows = 5000000 if DEBUG else None\n",
    "    #nrows = 12000000 if DEBUG else None\n",
    "    #nrows = 100000 if DEBUG else None\n",
    "    nrows = 300000 if DEBUG else None\n",
    "\n",
    "    dtype={'Asset_ID': 'int8', 'Count': 'int32', 'row_id': 'int32', 'Count': 'int32',\n",
    "           'Open': 'float32', 'High': 'float32', 'Low': 'float32', 'Close': 'float32',\n",
    "           'Volume': 'float32', 'VWAP': 'float32'}\n",
    "\n",
    "    if KAGGLE:\n",
    "      #train_df = pd.read_csv('../input/g-research-crypto-forecasting/train.csv', low_memory=False, dtype=dtype, nrows=nrows)\n",
    "      asset_details = pd.read_csv('../input/g-research-crypto-forecasting/asset_details.csv')\n",
    "      train_df = pd.read_parquet('../input/gresearch-recalc-trandata/train_recalc.parquet')\n",
    "      if DEBUG:\n",
    "        train_df = train_df.iloc[:nrows, :].reset_index(drop=True) \n",
    "\n",
    "    else:\n",
    "      #train_df = pd.read_csv('/content/drive/MyDrive/Machine_Learning/data/kaggle/2021_gresearch_crypto/01_input/train.csv', low_memory=False, dtype=dtype, nrows=nrows)\n",
    "      train_df = pd.read_parquet('/content/drive/MyDrive/Machine_Learning/data/kaggle/2021_gresearch_crypto/01_input/train_recalc_2201.parquet')\n",
    "      if DEBUG:\n",
    "        train_df = train_df.iloc[:nrows, :].reset_index(drop=True)\n",
    "    #filter to avoid time leakage with the data \n",
    "    filter_leakage = pd.to_datetime(train_df['timestamp'], unit='s') < '2021-06-13 00:00:00'\n",
    "    filter_memory = pd.to_datetime(train_df['timestamp'], unit='s') >= '2019-01-01 00:00:00'\n",
    "    \n",
    "    if LEAKAGE_FILTER:\n",
    "        train_df = train_df[filter_leakage]\n",
    "    \n",
    "    if YEAR_FILTER:\n",
    "        train_df = train_df[filter_memory]\n",
    "        train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(train_df.shape)\n",
    "    display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb0d2b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:14.241031Z",
     "iopub.status.busy": "2022-01-31T17:09:14.240365Z",
     "iopub.status.idle": "2022-01-31T17:09:21.307441Z",
     "shell.execute_reply": "2022-01-31T17:09:21.306892Z",
     "shell.execute_reply.started": "2022-01-31T15:09:27.875298Z"
    },
    "papermill": {
     "duration": 7.111754,
     "end_time": "2022-01-31T17:09:21.307592",
     "exception": false,
     "start_time": "2022-01-31T17:09:14.195838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-21 00:01:00\n",
      "2022-01-24 00:00:00\n",
      "(2518278, 10)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Asset_ID</th>\n",
       "      <th>Count</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1632182460</td>\n",
       "      <td>3</td>\n",
       "      <td>561</td>\n",
       "      <td>2.079028</td>\n",
       "      <td>2.080605</td>\n",
       "      <td>2.072000</td>\n",
       "      <td>2.076458</td>\n",
       "      <td>2.804627e+05</td>\n",
       "      <td>2.075869</td>\n",
       "      <td>0.003085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1632182460</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>541.005981</td>\n",
       "      <td>541.200012</td>\n",
       "      <td>539.700012</td>\n",
       "      <td>540.721985</td>\n",
       "      <td>1.889432e+02</td>\n",
       "      <td>540.716919</td>\n",
       "      <td>-0.000607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1632182460</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>363.737488</td>\n",
       "      <td>363.899994</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>363.653992</td>\n",
       "      <td>8.792685e+02</td>\n",
       "      <td>363.499542</td>\n",
       "      <td>-0.019375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1632182460</td>\n",
       "      <td>1</td>\n",
       "      <td>1933</td>\n",
       "      <td>42986.144531</td>\n",
       "      <td>43001.000000</td>\n",
       "      <td>42898.000000</td>\n",
       "      <td>42947.066406</td>\n",
       "      <td>9.183862e+01</td>\n",
       "      <td>42942.976562</td>\n",
       "      <td>-0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1632182460</td>\n",
       "      <td>4</td>\n",
       "      <td>348</td>\n",
       "      <td>0.208326</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.207800</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>1.051337e+06</td>\n",
       "      <td>0.208077</td>\n",
       "      <td>-0.001233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  Asset_ID  Count          Open          High           Low  \\\n",
       "0  1632182460         3    561      2.079028      2.080605      2.072000   \n",
       "1  1632182460         2    169    541.005981    541.200012    539.700012   \n",
       "2  1632182460         0    400    363.737488    363.899994    363.000000   \n",
       "3  1632182460         1   1933  42986.144531  43001.000000  42898.000000   \n",
       "4  1632182460         4    348      0.208326      0.208400      0.207800   \n",
       "\n",
       "          Close        Volume          VWAP    Target  \n",
       "0      2.076458  2.804627e+05      2.075869  0.003085  \n",
       "1    540.721985  1.889432e+02    540.716919 -0.000607  \n",
       "2    363.653992  8.792685e+02    363.499542 -0.019375  \n",
       "3  42947.066406  9.183862e+01  42942.976562 -0.000374  \n",
       "4      0.208200  1.051337e+06      0.208077 -0.001233  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281784\n"
     ]
    }
   ],
   "source": [
    "if SUPPLEMENT:\n",
    "    dtype={'Asset_ID': 'int8', 'Count': 'int32', 'row_id': 'int32', 'Count': 'int32',\n",
    "           'Open': 'float32', 'High': 'float32', 'Low': 'float32', 'Close': 'float32',\n",
    "           'Volume': 'float32', 'VWAP': 'float32'}\n",
    "    df_read  = pd.read_csv('../input/g-research-crypto-forecasting/supplemental_train.csv', dtype=dtype)\n",
    "    \n",
    "    if SUBTEST:\n",
    "        filter_leakage = pd.to_datetime(df_read['timestamp'], unit='s') < '2021-12-31 00:00:00'\n",
    "        filter_memory = pd.to_datetime(df_read['timestamp'], unit='s') >= '2021-09-01 00:00:00'\n",
    "        df_read2 = df_read[filter_memory]\n",
    "        df_read2 = df_read2[filter_leakage]\n",
    "        df_read2['timestamp'] = df_read2['timestamp'] + (3600 * 24 * 30*5)\n",
    "        #print(pd.to_datetime(df_read2.head(1)['timestamp'].values[0], unit='s'))\n",
    "        df_read = pd.concat([df_read, df_read2], ignore_index=True, copy=False)\n",
    "\n",
    "    if DEBUG:\n",
    "        nrows = 300000 if DEBUG else None\n",
    "        df_read  = df_read.iloc[:nrows, :]\n",
    "    \n",
    "    print(pd.to_datetime(df_read.head(1)['timestamp'].values[0], unit='s'))\n",
    "    print(pd.to_datetime(df_read.tail(1)['timestamp'].values[0], unit='s'))\n",
    "    print(df_read.shape)\n",
    "    print('='*100)\n",
    "    datasize = len(df_read)\n",
    "    diff_datasize = datasize - basesize\n",
    "    display(df_read.head())\n",
    "    print(diff_datasize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5977026e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:21.420136Z",
     "iopub.status.busy": "2022-01-31T17:09:21.419034Z",
     "iopub.status.idle": "2022-01-31T17:09:21.429734Z",
     "shell.execute_reply": "2022-01-31T17:09:21.429041Z",
     "shell.execute_reply.started": "2022-01-31T15:09:34.451292Z"
    },
    "papermill": {
     "duration": 0.072801,
     "end_time": "2022-01-31T17:09:21.429935",
     "exception": false,
     "start_time": "2022-01-31T17:09:21.357134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if KAGGLE: \n",
    "  asset_details = pd.read_csv('../input/g-research-crypto-forecasting/asset_details.csv')\n",
    "else:\n",
    "  asset_details = pd.read_csv('/content/drive/MyDrive/Machine_Learning/data/kaggle/2021_gresearch_crypto/01_input/asset_details.csv')\n",
    "\n",
    "#create dictionnary of weights\n",
    "dict_weights = {}\n",
    "for i in range(asset_details.shape[0]):\n",
    "    dict_weights[asset_details.iloc[i,0]] = asset_details.iloc[i,1]\n",
    "    \n",
    "weigths = np.array([dict_weights[i] for i in range(14)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a9a06fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:21.538606Z",
     "iopub.status.busy": "2022-01-31T17:09:21.537738Z",
     "iopub.status.idle": "2022-01-31T17:09:21.540970Z",
     "shell.execute_reply": "2022-01-31T17:09:21.541461Z",
     "shell.execute_reply.started": "2022-01-31T15:09:34.467381Z"
    },
    "papermill": {
     "duration": 0.066438,
     "end_time": "2022-01-31T17:09:21.541627",
     "exception": false,
     "start_time": "2022-01-31T17:09:21.475189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif SUPPLEMENT:\\n    # Generate the class/group data\\n\\n    import os\\n    time_ids = supple.timestamp.unique()\\n\\n    n_fold = 1\\n    splits = 0.6\\n    ntimes = len(time_ids)\\n\\n    embargo_train_test = 100 if DEBUG else 60*24*30\\n    embargo_fold = 100 if DEBUG else 60*24*30\\n\\n    time_per_fold = (ntimes - 5*embargo_train_test - 5*embargo_fold)/5\\n    train_len = splits*time_per_fold \\n    test_len = (1-splits)*time_per_fold\\n\\n    fold_start = [np.int(i*(len(time_ids)+1)/5) for i in range(6)]\\n\\n    for i in range(n_fold):\\n        time_folds = time_ids[fold_start[i]:fold_start[i+1]-1]\\n        df_fold = supple[supple.timestamp.isin(time_folds)]\\n        df_fold.to_parquet('df_fold_'+str(i)+'.parquet')\\n\\n    del supple\\n    gc.collect()\\n\\n    dict_fold = {}\\n\\n    for fold in range(n_fold):\\n        print('fold:'+str(fold))\\n\\n        df_fold = pd.read_parquet('df_fold_'+str(fold)+'.parquet')\\n        time_ids = df_fold.timestamp.unique()\\n\\n        test_train_len = len(time_ids) - embargo_train_test - embargo_fold\\n\\n        train_start = embargo_fold + 1\\n        train_end = embargo_fold + np.int(test_train_len*0.6) + 1\\n        test_start = embargo_fold + np.int(test_train_len*0.6) + embargo_train_test + 1\\n        test_end = len(df_fold.timestamp.unique())\\n\\n        dict_fold['train_fold_'+str(fold)] = time_ids[train_start:train_end]\\n        dict_fold['test_fold_'+str(fold)] = time_ids[test_start:test_end]\\n\\n    del df_fold\\n    gc.collect()\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if RETRAIN:\n",
    "    # Generate the class/group data\n",
    "\n",
    "    import os\n",
    "    time_ids = train_df.timestamp.unique()\n",
    "\n",
    "    n_fold = 5\n",
    "    splits = 0.6\n",
    "    ntimes = len(time_ids)\n",
    "\n",
    "    embargo_train_test = 100 if DEBUG else 60*24*30\n",
    "    embargo_fold = 100 if DEBUG else 60*24*30\n",
    "\n",
    "    time_per_fold = (ntimes - 5*embargo_train_test - 5*embargo_fold)/5\n",
    "    train_len = splits*time_per_fold \n",
    "    test_len = (1-splits)*time_per_fold\n",
    "\n",
    "    fold_start = [np.int(i*(len(time_ids)+1)/5) for i in range(6)]\n",
    "\n",
    "    for i in range(n_fold):\n",
    "        time_folds = time_ids[fold_start[i]:fold_start[i+1]-1]\n",
    "        df_fold = train_df[train_df.timestamp.isin(time_folds)]\n",
    "        df_fold.to_parquet('df_fold_'+str(i)+'.parquet')\n",
    "\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    dict_fold = {}\n",
    "\n",
    "    for fold in range(n_fold):\n",
    "        print('fold:'+str(fold))\n",
    "\n",
    "        df_fold = pd.read_parquet('df_fold_'+str(fold)+'.parquet')\n",
    "        time_ids = df_fold.timestamp.unique()\n",
    "\n",
    "        test_train_len = len(time_ids) - embargo_train_test - embargo_fold\n",
    "\n",
    "        train_start = embargo_fold + 1\n",
    "        train_end = embargo_fold + np.int(test_train_len*0.6) + 1\n",
    "        test_start = embargo_fold + np.int(test_train_len*0.6) + embargo_train_test + 1\n",
    "        test_end = len(df_fold.timestamp.unique())\n",
    "\n",
    "        dict_fold['train_fold_'+str(fold)] = time_ids[train_start:train_end]\n",
    "        dict_fold['test_fold_'+str(fold)] = time_ids[test_start:test_end]\n",
    "\n",
    "    del df_fold\n",
    "    gc.collect()\n",
    "\n",
    "'''\n",
    "if SUPPLEMENT:\n",
    "    # Generate the class/group data\n",
    "\n",
    "    import os\n",
    "    time_ids = supple.timestamp.unique()\n",
    "\n",
    "    n_fold = 1\n",
    "    splits = 0.6\n",
    "    ntimes = len(time_ids)\n",
    "\n",
    "    embargo_train_test = 100 if DEBUG else 60*24*30\n",
    "    embargo_fold = 100 if DEBUG else 60*24*30\n",
    "\n",
    "    time_per_fold = (ntimes - 5*embargo_train_test - 5*embargo_fold)/5\n",
    "    train_len = splits*time_per_fold \n",
    "    test_len = (1-splits)*time_per_fold\n",
    "\n",
    "    fold_start = [np.int(i*(len(time_ids)+1)/5) for i in range(6)]\n",
    "\n",
    "    for i in range(n_fold):\n",
    "        time_folds = time_ids[fold_start[i]:fold_start[i+1]-1]\n",
    "        df_fold = supple[supple.timestamp.isin(time_folds)]\n",
    "        df_fold.to_parquet('df_fold_'+str(i)+'.parquet')\n",
    "\n",
    "    del supple\n",
    "    gc.collect()\n",
    "\n",
    "    dict_fold = {}\n",
    "\n",
    "    for fold in range(n_fold):\n",
    "        print('fold:'+str(fold))\n",
    "\n",
    "        df_fold = pd.read_parquet('df_fold_'+str(fold)+'.parquet')\n",
    "        time_ids = df_fold.timestamp.unique()\n",
    "\n",
    "        test_train_len = len(time_ids) - embargo_train_test - embargo_fold\n",
    "\n",
    "        train_start = embargo_fold + 1\n",
    "        train_end = embargo_fold + np.int(test_train_len*0.6) + 1\n",
    "        test_start = embargo_fold + np.int(test_train_len*0.6) + embargo_train_test + 1\n",
    "        test_end = len(df_fold.timestamp.unique())\n",
    "\n",
    "        dict_fold['train_fold_'+str(fold)] = time_ids[train_start:train_end]\n",
    "        dict_fold['test_fold_'+str(fold)] = time_ids[test_start:test_end]\n",
    "\n",
    "    del df_fold\n",
    "    gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bdef317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:21.634602Z",
     "iopub.status.busy": "2022-01-31T17:09:21.633967Z",
     "iopub.status.idle": "2022-01-31T17:09:21.636934Z",
     "shell.execute_reply": "2022-01-31T17:09:21.637487Z",
     "shell.execute_reply.started": "2022-01-31T15:09:34.490415Z"
    },
    "papermill": {
     "duration": 0.052458,
     "end_time": "2022-01-31T17:09:21.637658",
     "exception": false,
     "start_time": "2022-01-31T17:09:21.585200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    }
   ],
   "source": [
    "# config\n",
    "MA_lags = [15, 240]#,60,240,1440]#,7*24*60,30*24*60]\n",
    "beta_lags = [60]#,7*24*60]#,30*24*60]\n",
    "print(len(MA_lags), len(beta_lags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25bd73bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:21.801753Z",
     "iopub.status.busy": "2022-01-31T17:09:21.761212Z",
     "iopub.status.idle": "2022-01-31T17:09:21.881076Z",
     "shell.execute_reply": "2022-01-31T17:09:21.881687Z",
     "shell.execute_reply.started": "2022-01-31T15:09:34.507175Z"
    },
    "papermill": {
     "duration": 0.200208,
     "end_time": "2022-01-31T17:09:21.881888",
     "exception": false,
     "start_time": "2022-01-31T17:09:21.681680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import deque\n",
    "\n",
    "class RunningMean:\n",
    "    def __init__(self, WIN_SIZE=20, n_size = 1):\n",
    "        self.n = 0\n",
    "        self.mean = np.zeros(n_size)\n",
    "        self.cum_max = 0\n",
    "        self.cum_sum = 0\n",
    "        self.past_value = 0\n",
    "        self.WIN_SIZE = WIN_SIZE\n",
    "        self.windows = collections.deque(maxlen=WIN_SIZE+1)\n",
    "        \n",
    "    def clear(self):\n",
    "        self.n = 0\n",
    "        self.windows.clear()\n",
    "\n",
    "    def push(self, x):\n",
    "        #currently fillna with past value, might want to change that\n",
    "        x = fillna_npwhere(x, self.past_value)\n",
    "        self.past_value = x\n",
    "        \n",
    "        self.windows.append(x)\n",
    "        self.cum_sum += x\n",
    "        \n",
    "        if self.n < self.WIN_SIZE:\n",
    "            self.n += 1\n",
    "            self.mean = self.cum_sum / float(self.n)\n",
    "            self.max = np.max(self.windows)\n",
    "            \n",
    "        else:\n",
    "            self.cum_sum -= self.windows.popleft()# 一番古いの削除\n",
    "            self.mean = self.cum_sum / float(self.WIN_SIZE)\n",
    "            #self.max = np.max(self.cum_sum)\n",
    "\n",
    "    def get_mean(self):\n",
    "        #print(windows.shape)\n",
    "        return self.mean if self.n else np.zeros(n_size)\n",
    "      \n",
    "    def get_max(self):\n",
    "        kara = []\n",
    "\n",
    "        a = len(self.windows[0])\n",
    "        #print(a)\n",
    "        for i in range(len(self.windows)):\n",
    " \n",
    "          kara.append(np.hstack(self.windows[i]))\n",
    "        #print(np.vstack(kara).shape)\n",
    "        return np.max(np.vstack(kara), axis=0).reshape(a, -1) if self.n else np.zeros(n_size)\n",
    "    \n",
    "    def get_min(self):\n",
    "        kara = []\n",
    "        #print(self.windows)\n",
    "        #print(\"=\"*100)\n",
    "        a = len(self.windows[0])\n",
    "        for i in range(len(self.windows)):\n",
    "          print(i)\n",
    "\n",
    "          kara.append(np.hstack(self.windows[i]))\n",
    "        #print(np.vstack(kara).shape)\n",
    "        return np.min(np.vstack(kara), axis=0).reshape(a, -1) if self.n else np.zeros(n_size)\n",
    "\n",
    "    def get_maxmin(self):\n",
    "        kara = []\n",
    "        a = len(self.windows[0]) #  ここはasetのかず14\n",
    "        \n",
    "        #print(a)\n",
    "        for i in range(len(self.windows)): # ここは全日数の話          \n",
    "          kara.append(np.hstack(self.windows[i]))\n",
    "        last_a = np.vstack(kara)\n",
    "\n",
    "        maxs = np.max(last_a, axis=0).reshape(a, -1) \n",
    "        mins =np.min(last_a, axis=0).reshape(a, -1)\n",
    "        return (maxs[:,1] - mins[:,2]).reshape(a, -1)  if self.n else np.zeros(n_size) # ここ用chk\n",
    "        # (np.transpose(np.array([HperL, GK_vol, RS_vol,log_r, mpower, base, baseh, basem, Dollars, Volume_per_trade, Dollars_per_trade])))\n",
    "\n",
    "    def get_realvol(self):\n",
    "        if len(self.windows) < 15:\n",
    "          a = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "          return np.concatenate((a, a), axis=1) if self.n else np.zeros(n_size)\n",
    "\n",
    "        # Features使う\n",
    "        vola = 3\n",
    "        a = len(self.windows[0])\n",
    "        #print(a)\n",
    "        band = np.arange(3, (len(self.windows))*6, 6) # ここ帰る\n",
    "        b = np.hstack(self.windows)\n",
    "        c =  np.sqrt(np.nansum(b[:, band[-16:-1]], axis=1)).astype(float).reshape(a, -1)\n",
    "        #print(c.shape)\n",
    "        if len(self.windows) < 240:\n",
    "          #print(xxx)\n",
    "          d = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "          return np.concatenate((c, d), axis=1) if self.n else np.zeros(n_size)\n",
    "        \n",
    "        else:\n",
    "          d =  np.sqrt(np.nansum(b[:, band], axis=1)).astype(float).reshape(a, -1)\n",
    "          return np.concatenate((c, d), axis=1) if self.n else np.zeros(n_size)\n",
    "\n",
    "    def get_lag(self):\n",
    "        # basic features使う\n",
    "        a = len(self.windows[0])\n",
    "        #print(len(self.windows))\n",
    "        band = np.arange(0, (len(self.windows))*9, 9) #np.arange(0, (2)*8, 8) ここ帰る\n",
    "        #print(self.windows[0].shape)\n",
    "        b = np.hstack(self.windows)\n",
    "        b = b[:, [band[0],band[-1]]]\n",
    "        return np.log(b[:,-1]/b[:,0]).reshape(a, -1) if self.n else np.zeros(n_size)\n",
    "    \n",
    "    def get_std(self):\n",
    "        # Features使う\n",
    "        a = len(self.windows[0])\n",
    "        #print(a)\n",
    "        band = np.arange(5, (len(self.windows))*6, 6) # ここ帰る\n",
    "        #print(self.windows[0].shape)\n",
    "        b = np.hstack(self.windows)\n",
    "        b = b[:, band]\n",
    "        return np.nanstd(b, axis=1).reshape(a, -1) if self.n else np.zeros(n_size)\n",
    "    \n",
    "    def rich_cldiff(self):\n",
    "\n",
    "        a = len(self.windows[0])\n",
    "        #print(a)\n",
    "        band1 = np.arange(6, (len(self.windows))*9 ,9) # ここ帰る\n",
    "\n",
    "        b = np.hstack(self.windows)\n",
    "        c = b[:, [band1[0],band1[-1]]]\n",
    "        diff_log = (c[:,-1] - c[:,0]).reshape(a, -1)\n",
    "        mean_diff_log = np.nanmean(diff_log)\n",
    "        mean_diff_log = np.expand_dims(np.array([mean_diff_log for i in range(14)]),axis=1)\n",
    "        \n",
    "        band2 = np.arange(7, (len(self.windows))*9, 9) \n",
    "        inv_weight_sum = 1 / np.nansum(b[:, band2[-1]])\n",
    "        #print(inv_weight_sum)\n",
    "        feature_cl_diff1_mean_w = np.nansum(diff_log * b[:, band2[-1]])* inv_weight_sum\n",
    "        feature_cl_diff1_mean_w = np.expand_dims(np.array([feature_cl_diff1_mean_w for i in range(14)]),axis=1)\n",
    "\n",
    "        market_return_causal = np.nansum(diff_log* b[:, band2[-1]]) * inv_weight_sum\n",
    "        market_return_causal = np.expand_dims(np.array([market_return_causal for i in range(14)]),axis=1)\n",
    "        #print(market_return_causal)\n",
    "        feature_cl_diff1_rank = np.argsort(np.argsort(np.hstack(diff_log))).reshape(a, -1)\n",
    "        #print(feature_cl_diff1_rank)\n",
    "        #return diff_log, mean_diff_log, inv_weight_sum,  feature_cl_diff1_mean_w, feature_cl_diff1_rank if self.n else np.zeros(n_size)\n",
    "        return (np.concatenate(((market_return_causal * diff_log), market_return_causal), axis=1), # rich feature 2個とcolse\n",
    "                   np.concatenate((diff_log, mean_diff_log, feature_cl_diff1_mean_w, feature_cl_diff1_rank),axis=1)\n",
    "                   if self.n else np.zeros(n_size) )# 'feature_cl_diff1', 'feature_cl_diff1_mean_simple', feature_cl_diff1_mean_simple', 'feature_cl_diff1_rank'\n",
    "\n",
    "    def rich_beta(self):\n",
    "        # (np.concatenate((beta_calc_features, baseinfo_features[:,0].reshape(14,1)), axis=1))\n",
    "        a = len(self.windows[0])\n",
    "  \n",
    "        b = np.hstack(self.windows)\n",
    "        #band1 = np.arange(0, (len(self.windows))*3, 3)\n",
    "        #band2 = np.arange(1, (len(self.windows))*3, 3)\n",
    "        # MAfeatures1\n",
    "        #band3 = np.arange(2, (len(self.windows))*3 ,3)          \n",
    "        ##############################################\n",
    "        # MA_Features2 enveroope\n",
    "        if len(self.windows) < 16:\n",
    "          \n",
    "          env_up = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "          env_low = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "          #chouki = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "          #print(env_low)\n",
    "\n",
    "        else:\n",
    "          env_std = np.nanstd(b[:, -16:-1], axis=1)\n",
    "          env_mean = np.nanmean(b[:, -16:-1], axis=1)\n",
    "          env_up = ((env_mean + env_std - b[:, -1]) / b[:, -1]).reshape(a, -1)\n",
    "          env_low = ((env_mean + (3*env_std) - b[:, -1]) / b[:, -1]).reshape(a, -1)\n",
    "        \n",
    "        #######################################################\n",
    "\n",
    "       \n",
    "\n",
    "        return (np.concatenate((env_up, env_low),axis=1) if self.n else np.zeros(n_size))# 'feature_cl_diff1', 'feature_cl_diff1_mean_simple', feature_cl_diff1_mean_simple', 'featu\n",
    "\n",
    "    ##############################################\n",
    "    def make_rsi(self):\n",
    "        a = len(self.windows[0])\n",
    "        #b = np.hstack(self.windows)\n",
    "\n",
    "    # RSI feature\n",
    "        if len(self.windows) < 240 * 15:\n",
    "          #print(len(self.windows))\n",
    "          \n",
    "          rsi15 = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "          rsi240 = rsi15#np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "          #print(self.windows[-1])\n",
    "          #print(\"=\"*100)\n",
    "        \n",
    "        else:\n",
    "          #baseline = b[:, band3[-15*15]]\n",
    "          #cols = [(i+1) * 14 for i in range(15)]\n",
    "          cols2 = [(i+1) * -15 for i in range(14)]\n",
    "          cols2 = cols2 + [-1]\n",
    "\n",
    "          #cols = [(i+1) * 240 for i in range(15)]\n",
    "          cols3 = [(i+1) * -240 for i in range(14)]\n",
    "          cols3= cols3 + [-1]\n",
    "\n",
    "          cols4 = [(i+1) * -60 for i in range(14)]\n",
    "          cols4= cols3 + [-1]\n",
    "          \n",
    "          b15 = []\n",
    "          b240 = []\n",
    "          #b60 = []\n",
    "          for i in range(len(cols2)):\n",
    "            #print(cols2[i])\n",
    "            #print(len(self.windows))\n",
    "            b15.append(self.windows[cols2[i]])\n",
    "            b240.append(self.windows[cols3[i]])\n",
    "            #b60.append(self.windows[cols4[i]])\n",
    "            \n",
    "          \n",
    "\n",
    "          #b15 = b[:, cols2]\n",
    "          b15 = np.hstack(b15)\n",
    "          b240 = np.hstack(b240)\n",
    "          #b60 = np.hstack(b60)\n",
    "          #print('A')\n",
    "          #b15 = b15[:, cols]\n",
    "          #print(b15)\n",
    "\n",
    "          #print(np.diff(b15, axis=1))\n",
    "          diff_15 = np.diff(b15, axis=1)\n",
    "          diff_240 = np.diff(b240, axis=1)\n",
    "          #zeros = np.expand_dims(np.array([0 for i in range(14)]),axis=1)\n",
    "\n",
    "          #print(np.where(diff_15 > 0, 0, diff_15))\n",
    "          #print(diff_15)\n",
    "          g = np.sum(np.where(diff_15 < 0, 0, diff_15), axis=1)\n",
    "          h = abs(np.sum(np.where(diff_15 > 0, 0, diff_15), axis=1))\n",
    "          i = np.sum(np.where(diff_240 < 0, 0, diff_240), axis=1)\n",
    "          j = abs(np.sum(np.where(diff_240 > 0, 0, diff_240), axis=1))\n",
    "\n",
    "          #print(g)\n",
    "          rsi15 = 100 * (g / (g + h)).reshape(a, -1)\n",
    "          rsi240 = 100 * (i / (i + j)).reshape(a, -1)\n",
    "         # macd\n",
    "        if len(self.windows) < 60 * 27:\n",
    "          macd15 = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "          macd60 = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "\n",
    "        else:\n",
    "          #baseline = b[:, band3[-15*15]]\n",
    "          #cols = [(i+1) * 14 for i in range(15)]\n",
    "          cols2 = [(i+1) * -15 for i in range(26)]\n",
    "          cols2 = cols2 + [-1]\n",
    "\n",
    "          #cols = [(i+1) * 240 for i in range(15)]\n",
    "          cols3 = [(i+1) * -60 for i in range(26)]\n",
    "          cols3= cols3 + [-1]\n",
    "            \n",
    "          #c15 = b[:, cols2]\n",
    "          #c60 = b[:, cols3]\n",
    "          \n",
    "          c15 = []\n",
    "          c60 = []\n",
    "          for i in range(len(cols2)):\n",
    "            #print(cols2[i])\n",
    "            #print(len(self.windows))\n",
    "            c15.append(self.windows[cols2[i]])\n",
    "            c60.append(self.windows[cols3[i]])\n",
    "\n",
    "          c15 = np.hstack(c15)\n",
    "          c60 = np.hstack(c60)\n",
    "          \n",
    "          macd15  = (np.nanmean(c15[:, -12:-1],axis=1) - np.nanmean(c15[:, -26:-1],axis=1)).reshape(a, -1)\n",
    "          macd60  = (np.nanmean(c60[:, -12:-1],axis=1) - np.nanmean(c60[:, -26:-1],axis=1)).reshape(a, -1)\n",
    "        \n",
    "        return np.concatenate((macd15, macd60), axis=1), (np.concatenate((rsi15, rsi240), axis=1) if self.n else np.zeros(n_size))\n",
    "    \n",
    "    def macd(self):\n",
    "      if len(self.windows) < 9:\n",
    "        \n",
    "        signal15 = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "        signal60 = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "      \n",
    "      else:\n",
    "        band1 = np.arange(0, (len(self.windows))*2, 2)\n",
    "        band2 = np.arange(1, (len(self.windows))*2, 2)\n",
    "\n",
    "        a = len(self.windows[0])\n",
    "        b = np.hstack(self.windows)\n",
    "\n",
    "        signal15 = np.nanmean(b[:, band1], axis=1).reshape(a, -1)\n",
    "        signal60 = np.nanmean(b[:, band2], axis=1).reshape(a, -1)\n",
    "\n",
    "        #hist15 = (b[:, -2].reshape(a, -1) - signal15).reshape(a, -1)\n",
    "        #hist60 = (b[:, -1].reshape(a, -1) - signal60).reshape(a, -1)\n",
    "\n",
    "        #mtanki = (signal15 > b[:, -2].reshape(a, -1)).astype(int).reshape(a,1)\n",
    "        #mchouki= (signal60 > b[:, -1].reshape(a, -1)).astype(int).reshape(a,1)\n",
    "\n",
    "      return (np.concatenate((signal15, signal60), axis=1) if self.n else np.zeros(n_size))\n",
    "\n",
    "    def vol_sum_std(self):\n",
    "        #band1 = np.arange(0, (len(self.windows))*7, 7)\n",
    "        band1 = np.arange(0, (len(self.windows))*2, 2)\n",
    "        a = len(self.windows[0])\n",
    "        if len(self.windows) < 16:\n",
    "          a = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "          return np.concatenate((a,a), axis=1)\n",
    "\n",
    "        else:\n",
    "          b = np.hstack(self.windows)\n",
    "          c = np.nansum(b[:,band1[-16:-1]], axis=1).reshape(a,1)\n",
    "          d = np.nansum(b[:,band1[-241:-1]], axis=1).reshape(a,1)\n",
    "\n",
    "          #e = np.nanstd(b[:,band1[-16:-1]], axis=1).reshape(a,1)\n",
    "          #f = np.nanstd(b[:,band1[-241:-1]], axis=1).reshape(a,1)\n",
    "        \n",
    "          #print(upper, upper.shape)\n",
    "          #print(downer, downer.shape)\n",
    "          return  np.concatenate((c,d), axis=1)if self.n else np.zeros(n_size)\n",
    "    \n",
    "\n",
    "    def mmean_std(self):\n",
    "      band1 = np.arange(0, (len(self.windows))*2, 2)\n",
    "      band2 = np.arange(1, (len(self.windows))*2, 2)\n",
    "      a = len(self.windows[0])\n",
    "      if len(self.windows) < 240:\n",
    "        a = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "        return np.concatenate((a, a, a), axis=1)  if self.n else np.zeros(n_size)\n",
    "\n",
    "      else:\n",
    "        b = np.vstack(self.windows)\n",
    "        #print(b)\n",
    "\n",
    "        c = np.nanmean(b[-16:-1, 0])#.reshape(a,1)\n",
    "        #print(c)\n",
    "        d = np.nanmean(b[-241:-1, 0])\n",
    "        #print(d)\n",
    "        #e = np.nanmean(b[-16:-1, 1])\n",
    "        #print(3)\n",
    "        #f = np.nanmean(b[-241:-1, 1])\n",
    "        \n",
    "        g = np.nanstd(b[-16:-1, 0])\n",
    "        #h = np.nanstd(b[-241:-1, 0])\n",
    "        #i = np.nanstd(b[-16:-1, 1])\n",
    "        #j = np.nanstd(b[-241:-1, 1])\n",
    "       \n",
    "        ##k = np.nansum(b[-16:-1, 0])\n",
    "        #l = np.nansum(b[-241:-1, 0])\n",
    "        #m = np.nansum(b[-16:-1, 1])\n",
    "        #n = np.nansum(b[-241:-1, 1])\n",
    "\n",
    "        #o = np.nanmean(b[-1, 0])\n",
    "        #p = np.nanmean(b[-1, 1])\n",
    "        #q = np.nansum(b[-1, 0])\n",
    "        #r = np.nansum(b[-1, 1])\n",
    "        xx = (np.array([c, d, g]))\n",
    "        #fin = np.vstack(np.array([xx for i in range(14)]))\n",
    "        fin = np.vstack(np.array([xx for i in range(14)]))\n",
    "        #print(fin)\n",
    "        return fin if self.n else np.zeros(n_size)\n",
    "\n",
    "    def vol_features(self):\n",
    "      band1 = np.arange(4, (len(self.windows))*9, 9) # vol\n",
    "      band2 = np.arange(5, (len(self.windows))*9, 9) # count\n",
    "      band3 = np.arange(0, (len(self.windows))*9, 9) # close\n",
    "\n",
    "      a = len(self.windows[0])\n",
    "      if len(self.windows) < 240:\n",
    "        a = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "        return np.concatenate((a, a), axis=1)  if self.n else np.zeros(n_size)\n",
    "        #return a if self.n else np.zeros(n_size)\n",
    "\n",
    "      else:\n",
    "        b = np.hstack(self.windows)\n",
    "        #print(b)\n",
    "        bs15 = np.max(b[:,band1[-16:-1]], axis=1).reshape(a,1)\n",
    "        bs240 = np.max(b[:,band1[-240:-1]], axis=1).reshape(a,1)\n",
    "\n",
    "        bcs15 = b[:,band3[-16]].reshape(a,1)\n",
    "        bcs240 = b[:,band3[-240]].reshape(a,1)\n",
    "\n",
    "        #print(bs15)\n",
    "        #print('='*100)\n",
    "        #print(b[:,band1[-16:-1]] )\n",
    "        #print('='*100)\n",
    "        #print(b[:,band1[-16:-1]] / bs15)\n",
    "\n",
    "        #b15sum = (np.nansum((b[:,band1[-16:-1]] / bs15), axis=1).reshape(a,1))\n",
    "        #b240sum = (np.nansum((b[:,band1[-240:-1]] / bs240), axis=1).reshape(a,1))\n",
    "        #b15mean = (np.nanmean((b[:,band1[-16:-1]] / bs15), axis=1).reshape(a,1))\n",
    "        #b240mean = (np.nanmean((b[:,band1[-240:-1]] / bs240), axis=1).reshape(a,1))\n",
    "        #b15std = np.nanstd((b[:,band1[-16:-1]]/ bs15), axis=1).reshape(a,1)\n",
    "        b240std = np.nanstd((b[:,band1[-240:-1]] / bs240), axis=1).reshape(a,1)\n",
    "\n",
    "        cs15 = np.max(b[:,band2[-16:-1]], axis=1).reshape(a,1)\n",
    "        cs240 = np.max(b[:,band2[-240:-1]], axis=1).reshape(a,1)\n",
    "\n",
    "        #c15sum = (np.nansum((b[:,band2[-16:-1]] / cs15), axis=1).reshape(a,1))\n",
    "        c240sum = (np.nansum((b[:,band2[-240:-1]] / cs240), axis=1).reshape(a,1))\n",
    "        #c15mean = (np.nanmean((b[:,band2[-16:-1]] / cs15), axis=1).reshape(a,1))\n",
    "        #c240mean = (np.nanmean((b[:,band2[-240:-1]] / cs240), axis=1).reshape(a,1))\n",
    "        #c15std = np.nanstd((b[:,band2[-16:-1]]/ cs15), axis=1).reshape(a,1)\n",
    "        #c240std = np.nanstd((b[:,band2[-240:-1]] / cs240), axis=1).reshape(a,1)\n",
    "\n",
    "        #d15sum = (np.nansum(((b[:,band1[-16:-1]] / bs15) / (b[:,band2[-16:-1]] / cs15)), axis=1).reshape(a,1))\n",
    "        #d240sum = (np.nansum(((b[:,band1[-240:-1]] / bs15) / (b[:,band2[-240:-1]] / cs240)), axis=1).reshape(a,1))\n",
    "        #d15mean = (np.nanmean(((b[:,band1[-16:-1]] / bs15) / (b[:,band2[-16:-1]] / cs15)), axis=1).reshape(a,1))\n",
    "        #d240mean = (np.nanmean(((b[:,band1[-240:-1]] / bs15) / (b[:,band2[-240:-1]] / cs240)), axis=1).reshape(a,1))\n",
    "        #d15std = np.nanstd(((b[:,band1[-16:-1]] / bs15) / (b[:,band2[-16:-1]]/ cs15)), axis=1).reshape(a,1)\n",
    "        #d240std = np.nanstd(((b[:,band1[-240:-1]] / bs15) / (b[:,band2[-240:-1]] / cs240)), axis=1).reshape(a,1)\n",
    "\n",
    "        #print(b15sum)\n",
    "        #e15 = (np.nansum(b[:,band3[-16:-1]] / bcs15, axis=1).reshape(a,1) * b15sum).reshape(a,1)\n",
    "        #e240 = (np.nansum(b[:,band3[-240:-1]] / bcs240, axis=1).reshape(a,1) * b240sum).reshape(a,1)\n",
    "        #print()\n",
    "        \n",
    "\n",
    "        return np.concatenate((b240std, c240sum), axis=1) if self.n else np.zeros(n_size)\n",
    "\n",
    "    def oth_teq(self):\n",
    "      #(tmp_df[[\"EndOfDayQuote PreviousClose\", \"EndOfDayQuote High\"]].max(axis=1) - tmp_df[[\"EndOfDayQuote PreviousClose\", \"EndOfDayQuote Low\"]].min(axis=1)) / tmp_df[\"EndOfDayQuote PreviousClose\"]\n",
    "      #tmp_df = tmp_df.replace([np.inf, -np.inf], np.nan)\n",
    "      band1 = np.arange(7, (len(self.windows))*9, 9) # open\n",
    "      band2 = np.arange(1, (len(self.windows))*9, 9) # high\n",
    "      band3 = np.arange(2, (len(self.windows))*9, 9) # min\n",
    "      band4 = np.arange(5, (len(self.windows))*9, 9) # vol\n",
    "      band5 = np.arange(0, (len(self.windows))*9, 9) # close\n",
    "      a = len(self.windows[0])\n",
    "\n",
    "      if len(self.windows) < 240:\n",
    "        a = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "        return np.concatenate((a, a, a, a), axis=1)  if self.n else np.zeros(n_size)\n",
    "\n",
    "      else:\n",
    "        b = np.hstack(self.windows)\n",
    "        #print(b)\n",
    "        #print(np.max(b[:,band2[-16:-1]], axis=1).reshape(a,1), np.min(b[:,band3[-16:-1]], axis=1).reshape(a,1), b[:,band1[-1]].reshape(a,1))\n",
    "        #print(\"=\"*100)\n",
    "        bs15 = ((np.nanmax(b[:,band2[-16:-1]], axis=1).reshape(a,1) - np.nanmin(b[:,band3[-16:-1]], axis=1).reshape(a,1)) / b[:,band1[-1]].reshape(a,1))\n",
    "        bs240 = ((np.nanmax(b[:,band2[-240:-1]], axis=1).reshape(a,1) - np.nanmin(b[:,band3[-240:-1]], axis=1).reshape(a,1)) / b[:,band1[-1]].reshape(a,1))\n",
    "\n",
    "        #print((np.sum(b[:, band4[-16:1]], axis=1) ))\n",
    "        mi15 = (bs15 / (np.nansum(b[:, band4[-16:-1]], axis=1) * np.nanmean(b[:, band5[-16:-1]], axis=1)).reshape(a,1)).reshape(a,1)\n",
    "        mi240 = (bs240 / (np.nansum(b[:, band4[-240:-1]], axis=1) * np.nanmean(b[:, band5[-240:-1]], axis=1)).reshape(a,1)).reshape(a,1)\n",
    "\n",
    "        return np.concatenate((bs15, bs240, mi15, mi240), axis=1) if self.n else np.zeros(n_size)\n",
    "\n",
    "    def tec_lags(self):\n",
    "      #(tmp_df[[\"EndOfDayQuote PreviousClose\", \"EndOfDayQuote High\"]].max(axis=1) - tmp_df[[\"EndOfDayQuote PreviousClose\", \"EndOfDayQuote Low\"]].min(axis=1)) / tmp_df[\"EndOfDayQuote PreviousClose\"]\n",
    "      #tmp_df = tmp_df.replace([np.inf, -np.inf2, np.nan)\n",
    "      band1 = np.arange(0, (len(self.windows))*9, 9) # open\n",
    "      band2 = np.arange(1, (len(self.windows))*9, 9) # high\n",
    "      band3 = np.arange(2, (len(self.windows))*9, 9) # min\n",
    "      band4 = np.arange(3, (len(self.windows))*9, 9) # vol\n",
    "      band5 = np.arange(4, (len(self.windows))*9, 9) # close\n",
    "      band6 = np.arange(5, (len(self.windows))*9, 9) \n",
    "      band7 = np.arange(6, (len(self.windows))*9, 9) \n",
    "      band8 = np.arange(7, (len(self.windows))*9, 9) \n",
    "      band9 = np.arange(8, (len(self.windows))*9, 9) \n",
    "      band10 = np.arange(9, (len(self.windows))*9, 9) \n",
    "\n",
    "      a = len(self.windows[0])\n",
    "\n",
    "      if len(self.windows) < 5:\n",
    "        a = np.expand_dims(np.array([np.nan for i in range(14)]),axis=1)\n",
    "        return np.concatenate((a,a,a,a,a,a,a,a,a,a,a,a,a,a,a,a,a,a,a), axis=1)  if self.n else np.zeros(n_size)\n",
    "\n",
    "      else:\n",
    "        b = np.hstack(self.windows)\n",
    "\n",
    "        l11 = b[:, band1[-1]].reshape(a, -1)\n",
    "        l12 = b[:, band2[-1]].reshape(a, -1)\n",
    "        l13 = b[:, band3[-1]].reshape(a, -1)\n",
    "        l14 = b[:, band4[-1]].reshape(a, -1)\n",
    "        l15 = b[:, band5[-1]].reshape(a, -1)\n",
    "        l16 = b[:, band6[-1]].reshape(a, -1)\n",
    "        l17 = b[:, band7[-1]].reshape(a, -1)\n",
    "        l18 = b[:, band8[-1]].reshape(a, -1)\n",
    "        l19 = b[:, band9[-1]].reshape(a, -1)\n",
    "        l110 = b[:, band10[-1]].reshape(a, -1)\n",
    "\n",
    "        l21 = b[:, band1[-2]].reshape(a, -1)\n",
    "        l22 = b[:, band2[-2]].reshape(a, -1)\n",
    "        l23 = b[:, band3[-2]].reshape(a, -1)\n",
    "        l24 = b[:, band4[-2]].reshape(a, -1)\n",
    "        l25 = b[:, band5[-2]].reshape(a, -1)\n",
    "        l26= b[:, band6[-2]].reshape(a, -1)\n",
    "        l27 = b[:, band7[-2]].reshape(a, -1)\n",
    "        l28 = b[:, band8[-2]].reshape(a, -1)\n",
    "        l29 = b[:, band9[-2]].reshape(a, -1)\n",
    "        #l210 = b[:, band10[-2]].reshape(a, -1)\n",
    "\n",
    "        #l31 = b[:, band1[-3]].reshape(a, -1)\n",
    "        #l32 = b[:, band2[-3]].reshape(a, -1)\n",
    "        #l33 = b[:, band3[-3]].reshape(a, -1)\n",
    "        #l34 = b[:, band4[-3]].reshape(a, -1)\n",
    "        #l35 = b[:, band5[-3]].reshape(a, -1)\n",
    "        #l36= b[:, band6[-3]].reshape(a, -1)\n",
    "        #l37 = b[:, band7[-3]].reshape(a, -1)\n",
    "        #l38 = b[:, band8[-3]].reshape(a, -1)\n",
    "        #l39 = b[:, band9[-3]].reshape(a, -1)\n",
    "        #l310 = b[:, band10[-3]].reshape(a, -1)\n",
    "\n",
    "        #l41 = b[:, band1[-4]].reshape(a, -1)\n",
    "        #l42 = b[:, band2[-4]].reshape(a, -1)\n",
    "        #l43 = b[:, band3[-4]].reshape(a, -1)\n",
    "        #l44 = b[:, band4[-4]].reshape(a, -1)\n",
    "        #l45 = b[:, band5[-4]].reshape(a, -1)\n",
    "        #l46= b[:, band6[-4]].reshape(a, -1)\n",
    "        #l47 = b[:, band7[-4]].reshape(a, -1)\n",
    "        #l48 = b[:, band8[-4]].reshape(a, -1)\n",
    "        #l49 = b[:, band9[-4]].reshape(a, -1)\n",
    "        #l410 = b[:, band10[-4]].reshape(a, -1)\n",
    "\n",
    "\n",
    "\n",
    "        return np.concatenate((l11,l12,l13,l14,l15,l16,l17,l18,l19,l110, l21,l22,l23,l24,l25,l26,l27,l28,l29,#l210, \n",
    "                                           #l31,l32,l33,l34,l35,l36,l37,l38,l39,l310, # l41,l42,l43,l44,l45,l46,l47,l48,l49,l410\n",
    "                              ), axis=1) if self.n else np.zeros(n_size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Current window values: {}\".format(list(self.windows))\n",
    "\n",
    "# Temporary removing njit as it cause many bugs down the line\n",
    "# Problems mainly due to data types, I have to find where I need to constraint types so as not to make njit angry\n",
    "\n",
    "def fillna_npwhere(array, values):\n",
    "    if np.isnan(array.sum()):\n",
    "        array = np.where(np.isnan(array), values, array)\n",
    "    return array\n",
    "\n",
    "def Clean_df(x):\n",
    "    Asset_ID = x[:,1]\n",
    "    timestamp = x[0,0]\n",
    "\n",
    "    if len(Asset_ID)<14:\n",
    "        missing_ID = [i for i in range(14) if i not in Asset_ID]\n",
    "        for i in missing_ID:\n",
    "            row = np.array((timestamp,i,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan))\n",
    "            x = np.concatenate((x,np.expand_dims(row,axis=0)))\n",
    "\n",
    "    x = x[np.argsort(x[:,1])]\n",
    "    return (x[:,i] for i in range(x.shape[1]))\n",
    "\n",
    "def Base_Feature_fn(timestamp,Asset_ID,Count,O,H,L,C,Volume,VWAP):\n",
    "\n",
    "    VWAP = np.where(np.isinf(VWAP),(C+O)/2,VWAP)\n",
    "    base = C\n",
    "    baseh = H                        # 追加\n",
    "    basem = L\n",
    "    baseo = O\n",
    "    HL = H/L\n",
    "    HperL = base / baseo\n",
    "    \n",
    "    O = O/base\n",
    "    H = H/base\n",
    "    L = L/base\n",
    "    C = C/base\n",
    "    VWAP = VWAP/C\n",
    "    Price = base\n",
    "\n",
    "    Dollars = Volume * Price\n",
    "    Volume_per_trade = Volume/Count\n",
    "    Dollars_per_trade = Dollars/Count\n",
    "\n",
    "    # 大口が買った (volume / count) * HL\n",
    "    mpower = HL#HL * (Volume / Count) # ここはHigh / lowに変更\n",
    "\n",
    "    log_ret = np.log(base/baseo) # 正しいターゲット\n",
    "    log_r = (np.log(Price) - np.log(baseo)) ** 2 # ralized vol計算用\n",
    "    GK_vol = (1 / 2 * np.log(H/L) ** 2 - (2 * np.log(2) - 1) * np.log(C/O) ** 2)\n",
    "    RS_vol = np.log(H/C)*np.log(H/O) + np.log(L/C)*np.log(L/O)\n",
    "    \n",
    "    # uki feature\n",
    "    #rang = baseh - basem\n",
    "    #hig_rang = ((baseh - basem) - np.abs(baseo - base)) / baseo\n",
    "    #mi = rang / Dollars\n",
    "\n",
    "    #return(np.transpose(np.array([Count,O,H,L,C,Price,Volume,VWAP,Dollars,Volume_per_trade,Dollars_per_trade,log_ret,GK_vol,RS_vol])))\n",
    "    #return(np.transpose(np.array([HL,Price, VWAP,Dollars,Volume_per_trade,Dollars_per_trade,log_ret,GK_vol,RS_vol,rang,hig_rang, mi])))\n",
    "    return(np.transpose(np.array([HperL, GK_vol, RS_vol,log_r, mpower, log_ret, base, baseh, basem, Dollars, Volume, Count,log_ret, baseo])))\n",
    "\n",
    "def Time_Feature_fn(timestamp):\n",
    "    \n",
    "    sin_month = (np.sin(2 * np.pi * timestamp.month/12))\n",
    "    cos_month = (np.cos(2 * np.pi * timestamp.month/12))\n",
    "    sin_day = (np.sin(2 * np.pi * timestamp.day/31))\n",
    "    cos_day = (np.cos(2 * np.pi * timestamp.day/31))\n",
    "    sin_hour = (np.sin(2 * np.pi * timestamp.hour/24))\n",
    "    cos_hour = (np.cos(2 * np.pi * timestamp.hour/24))\n",
    "    sin_minute = (np.sin(2 * np.pi * timestamp.minute/60))\n",
    "    cos_minute = (np.cos(2 * np.pi * timestamp.minute/60))\n",
    "\n",
    "    return(np.array((sin_month,cos_month,sin_day,cos_day,sin_hour,cos_hour,sin_minute,cos_minute)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23e430bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:21.976162Z",
     "iopub.status.busy": "2022-01-31T17:09:21.975457Z",
     "iopub.status.idle": "2022-01-31T17:09:22.044069Z",
     "shell.execute_reply": "2022-01-31T17:09:22.043366Z",
     "shell.execute_reply.started": "2022-01-31T15:09:34.658502Z"
    },
    "papermill": {
     "duration": 0.11762,
     "end_time": "2022-01-31T17:09:22.044234",
     "exception": false,
     "start_time": "2022-01-31T17:09:21.926614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if RETRAIN == False:\n",
    "    # read dict\n",
    "    \n",
    "    dict_RM = pd.read_pickle(os.path.join(savepath, '26_dict_RM.pkl'))\n",
    "    dict_RM_base = pd.read_pickle(os.path.join(savepath, '26_dict_RM_base.pkl'))\n",
    "    dict_RM_richbeta = pd.read_pickle(os.path.join(savepath, '26_dict_RM_richbeta.pkl'))\n",
    "    dict_RM_macd = pd.read_pickle(os.path.join(savepath, '26_dict_RM_macd.pkl'))\n",
    "    dict_RM_vols = pd.read_pickle(os.path.join(savepath, '26_dict_RM_vols.pkl'))\n",
    "    dict_RM_market = pd.read_pickle(os.path.join(savepath, '26_dict_RM_market.pkl'))\n",
    "    dict_RM_lag = pd.read_pickle(os.path.join(savepath, '26_dict_RM_lag.pkl'))\n",
    "    dict_RM_17 = pd.read_pickle(os.path.join(savepath, '26_dict_RM_17.pkl'))\n",
    "\n",
    "    #models = [pd.read_pickle(i) for i in glob.glob('../input/gresearch-35/lgbm__lightgbm.basic.Booster object at 0x7f9d23edd650_.pkl/*')]\n",
    "    #models = pd.read_pickle('../input/gresearch-37-est300/37_est300_lgbm__lightgbm.basic.Booster object at 0x7f3c3104cb90_.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d98803c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:22.139779Z",
     "iopub.status.busy": "2022-01-31T17:09:22.136489Z",
     "iopub.status.idle": "2022-01-31T17:09:22.155067Z",
     "shell.execute_reply": "2022-01-31T17:09:22.154370Z",
     "shell.execute_reply.started": "2022-01-31T15:09:34.757293Z"
    },
    "papermill": {
     "duration": 0.065816,
     "end_time": "2022-01-31T17:09:22.155230",
     "exception": false,
     "start_time": "2022-01-31T17:09:22.089414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "['HperL', 'log_r', 'log_ret', 'realized_vol_15', 'realized_vol_240', 'log_ret_15_lag', 'log_ret_15_std', 'ENV_up', 'ENV_do', 'RSI15', 'RSI240', 'macd60', 'signal15', 'signal60', 'realized_vol_std_sum_15', 'realized_vol_std_sum_240', 'm15_15mean', 'm15_240mean', 'm15_15std', 'vol240std', 'c240sum', 'log_ret_lag1', 'realized_vol_15_lag1', 'realized_vol_240_lag1', 'realized_vol_std_sum_15_lag1', 'realized_vol_std_sum_240_lag1', 'm15_15mean_lag1', 'm15_240mean_lag1', 'm15_15std_lag1', 'vol240std_lag1', 'c240sum_lag1', 'log_ret_lag2', 'realized_vol_15_lag2', 'realized_vol_240_lag2', 'realized_vol_std_sum_15_lag2', 'realized_vol_std_sum_240_lag2', 'm15_15mean_lag2', 'm15_240mean_lag2', 'm15_15std_lag2', 'vol240std_lag2']\n"
     ]
    }
   ],
   "source": [
    "Features_names = [#'Count','O',\n",
    "                 # 'HL',\n",
    "                  'HperL', #'Price',#'Volume',\n",
    "                 # 'VWAP','Dollars','Volume_per_trade','Dollars_per_trade',\n",
    "                  #'GK_vol','RS_vol', 'log_r', 'mpower', 'log_ret']#,'hig_rang','mi'\n",
    "                  'log_r', 'log_ret']#,'hig_rang','mi']\n",
    "base_names = ['Close', 'High', 'Low', 'Dollars', 'Volume_per_trade', 'Dollars_per_trade']\n",
    "Market_Features_names = [s+'_M' for s in Features_names]\n",
    "Time_Features_names = ['sin_month','cos_month','sin_day','cos_day','sin_hour','cos_hour','sin_minute','cos_minute']\n",
    "MA_Features_names = [s+'_'+str(lag) for lag in MA_lags for s in Features_names ]\n",
    "MA_Features_M_names = [s+'_'+str(lag) for lag in MA_lags for s in Market_Features_names]\n",
    "betas_names = ['betas_'+str(lag) for lag in beta_lags]\n",
    "\n",
    "MA_Featuresmax_names = [s+'_'+str(lag) + '_max' for lag in [15] for s in Features_names ]\n",
    "MA_Featuresmin_names = [s+'_'+str(lag) + '_min' for lag in [15] for s in Features_names ]\n",
    "Realized_vol_names = [s+'_'+str(lag)  for lag in [15, 240] for s in ['realized_vol'] ]\n",
    "MA_Featureslag_names = [s+'_'+str(lag) + '_lag' for lag in [15] for s in ['log_ret']]\n",
    "MA_Featuresstd_names = [s+'_'+str(lag) + '_std' for lag in [15] for s in ['log_ret']]\n",
    "\n",
    "Richman_Features_beta_name = [ 'ENV_up', 'ENV_do', 'RSI15', 'RSI240']\n",
    "macd_Features_name = ['macd60', 'signal15', 'signal60']\n",
    "#Btc_eth_corr_Features_name = ['Btc_corr', 'Eth_corr']\n",
    "#Updorate_name = ['Uprate_15', 'Dorate_15', 'Evenrate_15']\n",
    "Realized_vol_sum_names = [s+'_'+str(lag)  for lag in ['sum_15', 'sum_240'] for s in ['realized_vol_std'] ]\n",
    "market_Features_names =  ['m15_15mean', 'm15_240mean', 'm15_15std']\n",
    "vol_Features_names = ['vol240std', 'c240sum']\n",
    "\n",
    "lag_features_select = ['log_ret'] +  Realized_vol_names + Realized_vol_sum_names  + market_Features_names + vol_Features_names\n",
    "lag_features_select2 = [s + lag for lag in ['_lag1', '_lag2'] for s in lag_features_select]\n",
    "#oth_Features_name = ['hl_15', 'hl_240', 'mi15', 'mi240']\n",
    "\n",
    "All_names = (Features_names + Realized_vol_names  + MA_Featureslag_names +\n",
    "                    MA_Featuresstd_names + #Richman_Features_name + \n",
    "                    Richman_Features_beta_name + \n",
    "                    macd_Features_name + Realized_vol_sum_names +  market_Features_names + \n",
    "                    vol_Features_names + lag_features_select2) #+ oth_Features_name)#+ #Btc_eth_corr_Features_name + Updorate_name #+ MA_Featuresmaxmin_names #+ Market_Features_names + Time_Features_names + MA_Features_names + MA_Features_M_names + betas_names + MA_Featuresmax_names + MA_Featuresmin_names + Realized_vol_names\n",
    "\n",
    "All_names.remove('c240sum_lag2')\n",
    "print(len(All_names))\n",
    "print(All_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72753d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:22.255207Z",
     "iopub.status.busy": "2022-01-31T17:09:22.254373Z",
     "iopub.status.idle": "2022-01-31T17:09:22.258441Z",
     "shell.execute_reply": "2022-01-31T17:09:22.257911Z",
     "shell.execute_reply.started": "2022-01-31T15:09:34.774062Z"
    },
    "papermill": {
     "duration": 0.058795,
     "end_time": "2022-01-31T17:09:22.258578",
     "exception": false,
     "start_time": "2022-01-31T17:09:22.199783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "colabcols = [\n",
    "             'Asset_ID',\n",
    "             'HperL',\n",
    "             'log_r',\n",
    "             'log_ret',\n",
    "             'realized_vol_15',\n",
    "             'realized_vol_240',\n",
    "             'log_ret_15_lag',\n",
    "             'log_ret_15_std',\n",
    "             'ENV_up',\n",
    "             'ENV_do',\n",
    "             'RSI15',\n",
    "             'RSI240',\n",
    "             'macd60',\n",
    "             'signal15',\n",
    "             'signal60',\n",
    "             'realized_vol_std_sum_15',\n",
    "             'realized_vol_std_sum_240',\n",
    "             'm15_15mean',\n",
    "             'm15_240mean',\n",
    "             'm15_15std',\n",
    "             'vol240std',\n",
    "             'c240sum',\n",
    "             'log_ret_lag1',\n",
    "             'realized_vol_15_lag1',\n",
    "             'realized_vol_240_lag1',\n",
    "             'realized_vol_std_sum_15_lag1',\n",
    "             'realized_vol_std_sum_240_lag1',\n",
    "             'm15_15mean_lag1',\n",
    "             'm15_240mean_lag1',\n",
    "             'm15_15std_lag1',\n",
    "             'vol240std_lag1',\n",
    "             'c240sum_lag1',\n",
    "             'log_ret_lag2',\n",
    "             'realized_vol_15_lag2',\n",
    "             'realized_vol_240_lag2',\n",
    "             'realized_vol_std_sum_15_lag2',\n",
    "             'realized_vol_std_sum_240_lag2',\n",
    "             'm15_15mean_lag2',\n",
    "             'm15_240mean_lag2',\n",
    "             'm15_15std_lag2',\n",
    "             'vol240std_lag2'\n",
    "]\n",
    "\n",
    "colabcols_ridge = [\n",
    "                    #'Asset_ID',\n",
    "                    'log_r',\n",
    "                    'log_ret',\n",
    "                    'realized_vol_15',\n",
    "                    'realized_vol_240',\n",
    "                    'log_ret_15_lag',\n",
    "                    'ENV_up',\n",
    "                    'ENV_do',\n",
    "                    'm15_15mean',\n",
    "                    'm15_240mean',\n",
    "                    'm15_15std',\n",
    "                    'log_ret_lag1',\n",
    "                    'realized_vol_15_lag1',\n",
    "                    'realized_vol_240_lag1',\n",
    "                    'realized_vol_std_sum_15_lag1',\n",
    "                    'realized_vol_std_sum_240_lag1',\n",
    "                    'm15_15mean_lag1',\n",
    "                    'm15_240mean_lag1',\n",
    "                    'm15_15std_lag1',\n",
    "                    'vol240std_lag1',\n",
    "                    'log_ret_lag2',\n",
    "                    'realized_vol_15_lag2',\n",
    "                    'realized_vol_240_lag2',\n",
    "                    'realized_vol_std_sum_15_lag2',\n",
    "                    'realized_vol_std_sum_240_lag2',\n",
    "                    'm15_15mean_lag2',\n",
    "                    'm15_240mean_lag2',\n",
    "                    'm15_15std_lag2',\n",
    "                    'vol240std_lag2'            \n",
    "]\n",
    "\n",
    "print(len(colabcols))\n",
    "print(len(colabcols_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82253819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:22.355076Z",
     "iopub.status.busy": "2022-01-31T17:09:22.354115Z",
     "iopub.status.idle": "2022-01-31T17:09:22.359995Z",
     "shell.execute_reply": "2022-01-31T17:09:22.359326Z",
     "shell.execute_reply.started": "2022-01-31T15:09:34.79301Z"
    },
    "id": "UnvMrx9WXq2b",
    "papermill": {
     "duration": 0.056908,
     "end_time": "2022-01-31T17:09:22.360153",
     "exception": false,
     "start_time": "2022-01-31T17:09:22.303245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3745401188473625\n",
      "0.9507143064099162\n",
      "0.7319939418114051\n",
      "0.5986584841970366\n",
      "0.15601864044243652\n",
      "0.15599452033620265\n",
      "0.05808361216819946\n",
      "0.8661761457749352\n",
      "0.6011150117432088\n",
      "0.7080725777960455\n",
      "0.020584494295802447\n",
      "0.9699098521619943\n",
      "0.8324426408004217\n",
      "0.21233911067827616\n",
      "0.18182496720710062\n",
      "0.18340450985343382\n",
      "0.3042422429595377\n",
      "0.5247564316322378\n",
      "0.43194501864211576\n",
      "0.2912291401980419\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(42)\n",
    "np.random.seed(42)\n",
    "for j in range(2):\n",
    "    for i in range(10):\n",
    "        print(np.random.rand())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0fa8ff",
   "metadata": {
    "papermill": {
     "duration": 0.045988,
     "end_time": "2022-01-31T17:09:22.453286",
     "exception": false,
     "start_time": "2022-01-31T17:09:22.407298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Make train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "608e2a14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:09:22.561483Z",
     "iopub.status.busy": "2022-01-31T17:09:22.549211Z",
     "iopub.status.idle": "2022-01-31T17:23:54.086691Z",
     "shell.execute_reply": "2022-01-31T17:23:54.087199Z",
     "shell.execute_reply.started": "2022-01-31T15:09:34.813142Z"
    },
    "papermill": {
     "duration": 871.588407,
     "end_time": "2022-01-31T17:23:54.087384",
     "exception": false,
     "start_time": "2022-01-31T17:09:22.498977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "179996it [14:20, 209.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 3.969724 GB\n",
      "CPU times: user 14min 25s, sys: 11.2 s, total: 14min 37s\n",
      "Wall time: 14min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if RETRAIN == True:\n",
    "\n",
    "    # make train dataset\n",
    "    import os\n",
    "    from random import random\n",
    "\n",
    "    sampling = 1#0.05\n",
    "    #sampling = 0.8#0.05\n",
    "\n",
    "    counter = 0\n",
    "    np.random.seed(42)\n",
    "    for fold in range(n_fold):\n",
    "\n",
    "        df_train_fold = pd.DataFrame()\n",
    "        df_test_fold = pd.DataFrame()\n",
    "\n",
    "        df_read = pd.read_parquet(\"df_fold_\"+str(fold)+'.parquet')\n",
    "\n",
    "\n",
    "        #instantiation Moving average features dict\n",
    "        dict_RM = {}\n",
    "        dict_RM_M = {}\n",
    "        dict_RM_base = {}\n",
    "        dict_RM_richbeta = {}\n",
    "        dict_RM_macd = {}\n",
    "        dict_RM_vols = {}\n",
    "        dict_RM_market = {}\n",
    "        dict_RM_lag = {}\n",
    "        dict_RM_17 = {}\n",
    "\n",
    "\n",
    "        for lag in MA_lags:\n",
    "            dict_RM[lag] = RunningMean(lag)\n",
    "            dict_RM_M[lag] = RunningMean(lag)\n",
    "            dict_RM_base[lag] = RunningMean(lag)\n",
    "            dict_RM_market[lag] = RunningMean(lag)\n",
    "\n",
    "\n",
    "        dict_RM_richbeta[3601] = RunningMean(3601)\n",
    "        dict_RM_macd[9] = RunningMean(9)\n",
    "        dict_RM_lag[5] = RunningMean(5)\n",
    "        dict_RM_17[17] = RunningMean(17)\n",
    "\n",
    "        for lag in [15, 240]:\n",
    "          dict_RM_vols[lag] = RunningMean(lag)\n",
    "\n",
    "        #instantiation dict betas\n",
    "        #dict_MM = {}\n",
    "        #dict_Mr = {}\n",
    "        #for lag in beta_lags:\n",
    "        #    dict_MM[lag] = RunningMean(lag)\n",
    "        #    dict_Mr[lag] = RunningMean(lag)\n",
    "\n",
    "        #f = ['timestamp','Asset_ID','Count','Open','High','Low','Close','Volume','VWAP','Target']\n",
    "        f = ['timestamp','Asset_ID','Count','Open','High','Low','Close','Volume','VWAP','Re_Calc_Target']\n",
    "\n",
    "        t = df_read['timestamp'].values\n",
    "        ids, index = np.unique(t, return_index=True)\n",
    "        Values = df_read[f].values\n",
    "        splits = np.split(Values, index[1:])\n",
    "        out = []\n",
    "\n",
    "        kara = []\n",
    "        for time_id, x in tqdm(zip(ids.tolist(), splits)):\n",
    "\n",
    "            timestamp,Asset_ID,Count,O,H,L,C,Volume, VWAP,Target = Clean_df(x)\n",
    "\n",
    "            Features = Base_Feature_fn(timestamp,Asset_ID,Count,O,H,L,C,Volume,VWAP)\n",
    "            baseinfo_features = np.delete(Features, np.s_[0:6], axis=1)\n",
    "            Features = np.delete(Features, np.s_[-8:], axis=1)        \n",
    "\n",
    "            #removing wieghts when data is missing so that they don't appears in market\n",
    "            weigthss = np.where(np.isnan(O),O,weigths) # ここ変えた\n",
    "            #Market_Features = np.nansum(Features*np.expand_dims(weigths,axis=1)/np.nansum(weigths),axis=0)\n",
    "            #Market_Features = np.tile(Market_Features,(14,1))\n",
    "\n",
    "            #np.array((sin_month,cos_month,sin_day,cos_day,sin_hour,cos_hour,sin_minute,cos_minute))\n",
    "            time = timestamp_to_date(timestamp[0])\n",
    "            Time_Features = Time_Feature_fn(time)\n",
    "            #Time_Features = np.tile(Time_Features,(14,1))\n",
    "\n",
    "            MA_Features = []\n",
    "            #MA_Features_M  = []\n",
    "            #MA_Features_max = [] # max featur\n",
    "            #MA_Features_min = [] # min feature\n",
    "            MA_Features_maxmin = []\n",
    "            RealV_Feature = []\n",
    "            RealV_Feature_15240 = []\n",
    "            last_log_r_Feature = []\n",
    "            Std_Features = []\n",
    "            Richman_Features = []\n",
    "            Richman_Features_beata = []\n",
    "            Btc_eth_corr_Features = []\n",
    "            Updo_rate_Features = []\n",
    "            MACD_Featuees = []\n",
    "            Market_Features = []\n",
    "            Volume_Features = []\n",
    "            Other_teq_Features = []\n",
    "            teq_lagfeatures = []\n",
    "            RSI_features = []\n",
    "\n",
    "            for lag in MA_lags:\n",
    "                dict_RM[lag].push(Features)\n",
    "                #dict_RM_M[lag].push(Market_Features)\n",
    "                dict_RM_base[lag].push(np.concatenate((baseinfo_features, weigthss.reshape(14,1)), axis=1)) # ここ変えた\n",
    "                #dict_RM_richbeta\n",
    "                #MA_Features.append(dict_RM[lag].get_mean())\n",
    "                #MA_Features_M.append(dict_RM_M[lag].get_mean())\n",
    "                #RealV_Feature.append(dict_RM[lag].get_realvol()) \n",
    "\n",
    "\n",
    "            last_log_r_Feature.append(dict_RM_base[15].get_lag()) # ここ\n",
    "            Std_Features.append(dict_RM[15].get_std())\n",
    "\n",
    "            beta_calc_features, rich_features = dict_RM_base[15].rich_cldiff() # richman feature1　ここ\n",
    "            Richman_Features.append(rich_features)\n",
    "\n",
    "\n",
    "            dict_RM_17[17].push(baseinfo_features[:,0].reshape(14,1))\n",
    "            rich_tech_fe, macd_fe = dict_RM_17[17].rich_beta()\n",
    "            Richman_Features_beata.append(rich_tech_fe)\n",
    "            #print('A')\n",
    "            dict_RM_richbeta[3601].push(baseinfo_features[:,0].reshape(14,1)) # richman feature2\n",
    "            RSI_features.append(dict_RM_richbeta[3601].make_rsi())\n",
    "            #print('A')\n",
    "            #rich_tech_fe, macd_fe = dict_RM_richbeta[3601].rich_beta()\n",
    "\n",
    "\n",
    "\n",
    "            #print(Richman_Features_beata)\n",
    "\n",
    "            dict_RM_macd[9].push(macd_fe)\n",
    "            #print('A')\n",
    "            MACD_Featuees.append(np.concatenate((np.delete(macd_fe, np.s_[0], axis=1), dict_RM_macd[9].macd()), axis=1))\n",
    "            #print('A')\n",
    "\n",
    "            #Btc_eth_corr_Features.append(dict_RM_base[15].corr_btc_eth())\n",
    "            #Updo_rate_Features.append(dict_RM_base[15].updown_rate())\n",
    "\n",
    "            RealV_Feature.append(dict_RM[240].get_realvol())  # vola\n",
    "            #print('A')\n",
    "\n",
    "            dict_RM_vols[240].push(dict_RM[15].get_realvol())\n",
    "            RealV_Feature_15240.append(dict_RM_vols[240].vol_sum_std())\n",
    "\n",
    "            market_mean = np.nanmean(Features[:,5])\n",
    "            market_mean_abs = np.nanmean(abs(Features[:,5]))\n",
    "            dict_RM_market[240].push(np.array([market_mean, market_mean_abs]))\n",
    "            Market_Features.append(dict_RM_market[240].mmean_std())\n",
    "\n",
    "            Volume_Features.append(dict_RM_base[240].vol_features())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #Other_teq_Features.append(dict_RM_base[240].oth_teq())\n",
    "            #print('zzz')\n",
    "            #RealV_Feature.append(dict_RM[60].get_realvol())\n",
    "\n",
    "            #for term in [60, 240, 1440]:\n",
    "            #  MA_Features_maxmin.append(dict_RM_base[term].get_maxmin())# maxmin\n",
    "              #print('A')\n",
    "\n",
    "\n",
    "            #print('A')\n",
    "            #MA_Features = np.concatenate(MA_Features,axis=1)\n",
    "            #print('B')\n",
    "            #MA_Features_M = np.concatenate(MA_Features_M)\n",
    "            #MA_Features_maxmin = np.concatenate(MA_Features_maxmin,axis=1) # maxmin\n",
    "            #print('C')\n",
    "\n",
    "            #MA_Features_max = np.concatenate(MA_Features_max,axis=1)\n",
    "            #MA_Features_min = np.concatenate(MA_Features_min,axis=1)\n",
    "            RealV_Feature = np.concatenate(RealV_Feature,axis=1) # vola\n",
    "\n",
    "            last_log_r_Feature = np.concatenate(last_log_r_Feature,axis=1) # lag\n",
    "            Std_Features = np.concatenate(Std_Features,axis=1) # 標準偏差\n",
    "\n",
    "            #Richman_Features = np.concatenate(Richman_Features, axis=1) # richman feature1\n",
    "            Richman_Features_beata = np.concatenate(Richman_Features_beata, axis=1) # richman feature1\n",
    "            RSI_features = np.concatenate(RSI_features, axis=1)\n",
    "            MACD_Featuees = np.concatenate(MACD_Featuees, axis=1)\n",
    "\n",
    "            RealV_Feature_15240 = np.concatenate(RealV_Feature_15240, axis=1)\n",
    "\n",
    "            Market_Features = np.concatenate(Market_Features, axis=1)\n",
    "            Volume_Features = np.concatenate(Volume_Features, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            dict_RM_lag[5].push(np.concatenate((np.delete(Features, np.s_[:-1], axis=1),  RealV_Feature, RealV_Feature_15240, Market_Features, Volume_Features), axis=1))\n",
    "            #print('A')\n",
    "            teq_lagfeatures.append(dict_RM_lag[5].tec_lags())\n",
    "            teq_lagfeatures = np.concatenate(teq_lagfeatures, axis=1)\n",
    "\n",
    "            #Other_teq_Features = np.concatenate(Other_teq_Features, axis=1)\n",
    "            #break\n",
    "            #print('A')\n",
    "            #Btc_eth_corr_Features = np.concatenate(Btc_eth_corr_Features, axis=1)\n",
    "            #Updo_rate_Features = np.concatenate(Updo_rate_Features, axis=1)\n",
    "\n",
    "\n",
    "            #print('A')\n",
    "            #MA_Features_M = np.tile(MA_Features_M,(14,1))\n",
    "            #print('A')\n",
    "            betas = []\n",
    "            #break\n",
    "            #for lag in beta_lags:\n",
    "            #    dict_MM[lag].push(Market_Features[1]**2)\n",
    "            #    dict_Mr[lag].push(Market_Features[1]*Features[:,1]) # ここ注意\n",
    "            #    betas.append(np.expand_dims(dict_Mr[lag].get_mean()/dict_MM[lag].get_mean(),axis=1))\n",
    "            #break\n",
    "            #print('A')\n",
    "            Features = np.delete(Features, np.s_[1:3], axis=1)\n",
    "            Features = np.delete(Features, 2, axis=1)\n",
    "            #print('C')\n",
    "            #betas = np.concatenate(betas,axis=1)\n",
    "            #betas = np.nan_to_num(betas, nan=0., posinf=0., neginf=0.) \n",
    "            #Features = np.delete(Features, 4, axis=1)\n",
    "            #values = np.concatenate((Features,np.tile(Market_Features,(14,1)),np.tile(Time_Features,(14,1)),MA_Features,np.tile(MA_Features_M,(14,1)),betas, MA_Features_max, MA_Features_min, RealV_Feature, np.expand_dims(Target,axis=1)),axis=1)\n",
    "            #values = np.concatenate((Features,RealV_Feature,betas,MA_Features, MA_Features_maxmin,np.expand_dims(Target,axis=1)),axis=1)\n",
    "            #values = np.concatenate((Features,RealV_Feature,last_log_r_Feature, Std_Features, Richman_Features,Richman_Features_beata, MACD_Featuees, Btc_eth_corr_Features, Updo_rate_Features, np.expand_dims(Target,axis=1)),axis=1)\n",
    "            values = np.concatenate((Features,RealV_Feature,last_log_r_Feature, Std_Features, #Richman_Features,\n",
    "                                                  Richman_Features_beata, RSI_features, \n",
    "                                                  MACD_Featuees, RealV_Feature_15240, Market_Features, \n",
    "                                                  Volume_Features, #Other_teq_Features, \n",
    "                                                  teq_lagfeatures, \n",
    "                                                  np.expand_dims(Target,axis=1)),axis=1)\n",
    "            #break\n",
    "            #print('B')\n",
    "            if np.random.rand() < sampling:\n",
    "                out.append(np.concatenate((np.expand_dims(timestamp,axis=1),np.expand_dims(Asset_ID,axis=1),np.float32(values)),axis=1))\n",
    "        #print(values.shape)\n",
    "        #print(len(out))\n",
    "        #df_out = pd.DataFrame(np.concatenate(out), columns = ['timestamp','Asset_ID'] + All_names + ['Target']).astype({'timestamp': 'int64','Asset_ID': 'int64'})\n",
    "        train = pd.DataFrame(np.concatenate(out), columns = ['timestamp','Asset_ID'] + All_names + ['Target']).astype({'timestamp': 'int64','Asset_ID': 'int64'})\n",
    "        train = train[~np.isnan(train.Target)]\n",
    "        #df_out = df_out[~np.isnan(df_out.Target)]\n",
    "        train.to_parquet('train_fold_'+str(fold)+'.parquet')\n",
    "        print('before:', peak_memory()/10**6, 'GB')\n",
    "\n",
    "        #df_test_fold.to_parquet('test_fold_'+str(fold)+'.parquet')\n",
    "        del train\n",
    "        gc.collect()\n",
    "        #ind_train = df_out.timestamp.isin(dict_fold['train_fold_'+str(fold)])\n",
    "        #ind_test = df_out.timestamp.isin(dict_fold['test_fold_'+str(fold)])\n",
    "\n",
    "        #df_train_fold = df_out[ind_train]\n",
    "        #df_test_fold = df_out[ind_test]\n",
    "\n",
    "        #df_train_fold['fold'] = fold\n",
    "        #df_test_fold['fold'] = fold\n",
    "        '''\n",
    "        if counter == 0:\n",
    "            del df_read\n",
    "            gc.collect()\n",
    "            #print(df_train_fold.head())\n",
    "            train = df_train_fold.copy()\n",
    "            del df_train_fold\n",
    "            gc.collect()\n",
    "\n",
    "            valid = df_test_fold.copy()\n",
    "            del df_test_fold\n",
    "            gc.collect()\n",
    "\n",
    "            counter += 1\n",
    "            #print('A')\n",
    "\n",
    "        else:\n",
    "            del df_read\n",
    "            gc.collect()\n",
    "            #print('B')\n",
    "            train = pd.concat([train, df_train_fold]).reset_index(drop=True)\n",
    "            valid = pd.concat([valid, df_test_fold]).reset_index(drop=True)\n",
    "\n",
    "            del df_train_fold, df_test_fold\n",
    "            gc.collect()\n",
    "\n",
    "        #df_train_fold.to_parquet('train_fold_'+str(fold)+'.parquet')\n",
    "        #df_test_fold.to_parquet('test_fold_'+str(fold)+'.parquet')\n",
    "        '''\n",
    "\n",
    "if SUPPLEMENT == True:\n",
    "\n",
    "\n",
    "    # make train dataset\n",
    "    import os\n",
    "    from random import random\n",
    "\n",
    "    sampling = 1#0.05\n",
    "    #sampling = 0.8#0.05\n",
    "    n_fold = 1\n",
    "\n",
    "    counter = 0\n",
    "    np.random.seed(42)\n",
    "    for fold in range(n_fold):\n",
    "\n",
    "        df_train_fold = pd.DataFrame()\n",
    "        df_test_fold = pd.DataFrame()\n",
    "\n",
    "        #df_read = pd.read_parquet(\"df_fold_\"+str(fold)+'.parquet')\n",
    "\n",
    "\n",
    "        #instantiation dict betas\n",
    "        #dict_MM = {}\n",
    "        #dict_Mr = {}\n",
    "        #for lag in beta_lags:\n",
    "        #    dict_MM[lag] = RunningMean(lag)\n",
    "        #    dict_Mr[lag] = RunningMean(lag)\n",
    "\n",
    "        #f = ['timestamp','Asset_ID','Count','Open','High','Low','Close','Volume','VWAP','Target']\n",
    "        f = ['timestamp','Asset_ID','Count','Open','High','Low','Close','Volume','VWAP','Target']\n",
    "\n",
    "        t = df_read['timestamp'].values\n",
    "        ids, index = np.unique(t, return_index=True)\n",
    "        Values = df_read[f].values\n",
    "        splits = np.split(Values, index[1:])\n",
    "        out = []\n",
    "\n",
    "        kara = []\n",
    "        for time_id, x in tqdm(zip(ids.tolist(), splits)):\n",
    "\n",
    "            timestamp,Asset_ID,Count,O,H,L,C,Volume, VWAP,Target = Clean_df(x)\n",
    "\n",
    "            Features = Base_Feature_fn(timestamp,Asset_ID,Count,O,H,L,C,Volume,VWAP)\n",
    "            baseinfo_features = np.delete(Features, np.s_[0:6], axis=1)\n",
    "            Features = np.delete(Features, np.s_[-8:], axis=1)        \n",
    "\n",
    "            #removing wieghts when data is missing so that they don't appears in market\n",
    "            weigthss = np.where(np.isnan(O),O,weigths) # ここ変えた\n",
    "            #Market_Features = np.nansum(Features*np.expand_dims(weigths,axis=1)/np.nansum(weigths),axis=0)\n",
    "            #Market_Features = np.tile(Market_Features,(14,1))\n",
    "\n",
    "            #np.array((sin_month,cos_month,sin_day,cos_day,sin_hour,cos_hour,sin_minute,cos_minute))\n",
    "            time = timestamp_to_date(timestamp[0])\n",
    "            Time_Features = Time_Feature_fn(time)\n",
    "            #Time_Features = np.tile(Time_Features,(14,1))\n",
    "\n",
    "            MA_Features = []\n",
    "            #MA_Features_M  = []\n",
    "            #MA_Features_max = [] # max featur\n",
    "            #MA_Features_min = [] # min feature\n",
    "            MA_Features_maxmin = []\n",
    "            RealV_Feature = []\n",
    "            RealV_Feature_15240 = []\n",
    "            last_log_r_Feature = []\n",
    "            Std_Features = []\n",
    "            Richman_Features = []\n",
    "            Richman_Features_beata = []\n",
    "            Btc_eth_corr_Features = []\n",
    "            Updo_rate_Features = []\n",
    "            MACD_Featuees = []\n",
    "            Market_Features = []\n",
    "            Volume_Features = []\n",
    "            Other_teq_Features = []\n",
    "            teq_lagfeatures = []\n",
    "            RSI_features = []\n",
    "\n",
    "            for lag in MA_lags:\n",
    "                dict_RM[lag].push(Features)\n",
    "                #dict_RM_M[lag].push(Market_Features)\n",
    "                dict_RM_base[lag].push(np.concatenate((baseinfo_features, weigthss.reshape(14,1)), axis=1)) # ここ変えた\n",
    "                #dict_RM_richbeta\n",
    "                #MA_Features.append(dict_RM[lag].get_mean())\n",
    "                #MA_Features_M.append(dict_RM_M[lag].get_mean())\n",
    "                #RealV_Feature.append(dict_RM[lag].get_realvol()) \n",
    "\n",
    "\n",
    "            last_log_r_Feature.append(dict_RM_base[15].get_lag()) # ここ\n",
    "            Std_Features.append(dict_RM[15].get_std())\n",
    "\n",
    "            beta_calc_features, rich_features = dict_RM_base[15].rich_cldiff() # richman feature1　ここ\n",
    "            Richman_Features.append(rich_features)\n",
    "\n",
    "\n",
    "            dict_RM_17[17].push(baseinfo_features[:,0].reshape(14,1))\n",
    "            rich_tech_fe = dict_RM_17[17].rich_beta()\n",
    "            Richman_Features_beata.append(rich_tech_fe)\n",
    "            #print('A')\n",
    "\n",
    "            dict_RM_richbeta[3601].push(baseinfo_features[:,0].reshape(14,1)) # richman feature2\n",
    "            macd_fe, rsi_fe = dict_RM_richbeta[3601].make_rsi()\n",
    "            RSI_features.append(rsi_fe)\n",
    "            #print('A')\n",
    "            #rich_tech_fe, macd_fe = dict_RM_richbeta[3601].rich_beta()\n",
    "\n",
    "\n",
    "\n",
    "            #print(Richman_Features_beata)\n",
    "\n",
    "            dict_RM_macd[10].push(macd_fe)\n",
    "            #print('A')\n",
    "            MACD_Featuees.append(np.concatenate((np.delete(macd_fe, np.s_[0], axis=1), dict_RM_macd[10].macd()), axis=1))\n",
    "            #print('A')\n",
    "\n",
    "            #Btc_eth_corr_Features.append(dict_RM_base[15].corr_btc_eth())\n",
    "            #Updo_rate_Features.append(dict_RM_base[15].updown_rate())\n",
    "\n",
    "            RealV_Feature.append(dict_RM[240].get_realvol())  # vola\n",
    "            #print('A')\n",
    "\n",
    "            dict_RM_vols[240].push(dict_RM[15].get_realvol())\n",
    "            RealV_Feature_15240.append(dict_RM_vols[240].vol_sum_std())\n",
    "\n",
    "            market_mean = np.nanmean(Features[:,5])\n",
    "            market_mean_abs = np.nanmean(abs(Features[:,5]))\n",
    "            dict_RM_market[240].push(np.array([market_mean, market_mean_abs]))\n",
    "            Market_Features.append(dict_RM_market[240].mmean_std())\n",
    "\n",
    "            Volume_Features.append(dict_RM_base[240].vol_features())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #Other_teq_Features.append(dict_RM_base[240].oth_teq())\n",
    "            #print('zzz')\n",
    "            #RealV_Feature.append(dict_RM[60].get_realvol())\n",
    "\n",
    "            #for term in [60, 240, 1440]:\n",
    "            #  MA_Features_maxmin.append(dict_RM_base[term].get_maxmin())# maxmin\n",
    "              #print('A')\n",
    "\n",
    "\n",
    "            #print('A')\n",
    "            #MA_Features = np.concatenate(MA_Features,axis=1)\n",
    "            #print('B')\n",
    "            #MA_Features_M = np.concatenate(MA_Features_M)\n",
    "            #MA_Features_maxmin = np.concatenate(MA_Features_maxmin,axis=1) # maxmin\n",
    "            #print('C')\n",
    "\n",
    "            #MA_Features_max = np.concatenate(MA_Features_max,axis=1)\n",
    "            #MA_Features_min = np.concatenate(MA_Features_min,axis=1)\n",
    "            RealV_Feature = np.concatenate(RealV_Feature,axis=1) # vola\n",
    "\n",
    "            last_log_r_Feature = np.concatenate(last_log_r_Feature,axis=1) # lag\n",
    "            Std_Features = np.concatenate(Std_Features,axis=1) # 標準偏差\n",
    "\n",
    "            #Richman_Features = np.concatenate(Richman_Features, axis=1) # richman feature1\n",
    "            Richman_Features_beata = np.concatenate(Richman_Features_beata, axis=1) # richman feature1\n",
    "            RSI_features = np.concatenate(RSI_features, axis=1)\n",
    "            MACD_Featuees = np.concatenate(MACD_Featuees, axis=1)\n",
    "\n",
    "            RealV_Feature_15240 = np.concatenate(RealV_Feature_15240, axis=1)\n",
    "\n",
    "            Market_Features = np.concatenate(Market_Features, axis=1)\n",
    "            Volume_Features = np.concatenate(Volume_Features, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            dict_RM_lag[5].push(np.concatenate((np.delete(Features, np.s_[:-1], axis=1),  RealV_Feature, RealV_Feature_15240, Market_Features, Volume_Features), axis=1))\n",
    "            #print('A')\n",
    "            teq_lagfeatures.append(dict_RM_lag[5].tec_lags())\n",
    "            teq_lagfeatures = np.concatenate(teq_lagfeatures, axis=1)\n",
    "\n",
    "            #Other_teq_Features = np.concatenate(Other_teq_Features, axis=1)\n",
    "            #break\n",
    "            #print('A')\n",
    "            #Btc_eth_corr_Features = np.concatenate(Btc_eth_corr_Features, axis=1)\n",
    "            #Updo_rate_Features = np.concatenate(Updo_rate_Features, axis=1)\n",
    "\n",
    "\n",
    "            #print('A')\n",
    "            #MA_Features_M = np.tile(MA_Features_M,(14,1))\n",
    "            #print('A')\n",
    "            betas = []\n",
    "            #break\n",
    "            #for lag in beta_lags:\n",
    "            #    dict_MM[lag].push(Market_Features[1]**2)\n",
    "            #    dict_Mr[lag].push(Market_Features[1]*Features[:,1]) # ここ注意\n",
    "            #    betas.append(np.expand_dims(dict_Mr[lag].get_mean()/dict_MM[lag].get_mean(),axis=1))\n",
    "            #break\n",
    "            #print('A')\n",
    "            Features = np.delete(Features, np.s_[1:3], axis=1)\n",
    "            Features = np.delete(Features, 2, axis=1)\n",
    "            #print('C')\n",
    "            #betas = np.concatenate(betas,axis=1)\n",
    "            #betas = np.nan_to_num(betas, nan=0., posinf=0., neginf=0.) \n",
    "            #Features = np.delete(Features, 4, axis=1)\n",
    "            #values = np.concatenate((Features,np.tile(Market_Features,(14,1)),np.tile(Time_Features,(14,1)),MA_Features,np.tile(MA_Features_M,(14,1)),betas, MA_Features_max, MA_Features_min, RealV_Feature, np.expand_dims(Target,axis=1)),axis=1)\n",
    "            #values = np.concatenate((Features,RealV_Feature,betas,MA_Features, MA_Features_maxmin,np.expand_dims(Target,axis=1)),axis=1)\n",
    "            #values = np.concatenate((Features,RealV_Feature,last_log_r_Feature, Std_Features, Richman_Features,Richman_Features_beata, MACD_Featuees, Btc_eth_corr_Features, Updo_rate_Features, np.expand_dims(Target,axis=1)),axis=1)\n",
    "            values = np.concatenate((Features,RealV_Feature,last_log_r_Feature, Std_Features, #Richman_Features,\n",
    "                                                  Richman_Features_beata, RSI_features, \n",
    "                                                  MACD_Featuees, RealV_Feature_15240, Market_Features, \n",
    "                                                  Volume_Features, #Other_teq_Features, \n",
    "                                                  teq_lagfeatures, \n",
    "                                                  np.expand_dims(Target,axis=1)),axis=1)\n",
    "            #break\n",
    "            #print('B')\n",
    "            if np.random.rand() < sampling:\n",
    "                out.append(np.concatenate((np.expand_dims(timestamp,axis=1),np.expand_dims(Asset_ID,axis=1),np.float32(values)),axis=1))\n",
    "        #print(values.shape)\n",
    "        #print(len(out))\n",
    "        #df_out = pd.DataFrame(np.concatenate(out), columns = ['timestamp','Asset_ID'] + All_names + ['Target']).astype({'timestamp': 'int64','Asset_ID': 'int64'})\n",
    "        train = pd.DataFrame(np.concatenate(out), columns = ['timestamp','Asset_ID'] + All_names + ['Target']).astype({'timestamp': 'int64','Asset_ID': 'int64'})\n",
    "        train = train[~np.isnan(train.Target)]\n",
    "        #df_out = df_out[~np.isnan(df_out.Target)]\n",
    "        train.to_parquet('train_fold_'+str(fold)+'.parquet')\n",
    "        print('before:', peak_memory()/10**6, 'GB')\n",
    "        #df_test_fold.to_parquet('test_fold_'+str(fold)+'.parquet')\n",
    "        del train\n",
    "        gc.collect()\n",
    "        #ind_train = df_out.timestamp.isin(dict_fold['train_fold_'+str(fold)])\n",
    "        #ind_test = df_out.timestamp.isin(dict_fold['test_fold_'+str(fold)])\n",
    "\n",
    "        #df_train_fold = df_out[ind_train]\n",
    "        #df_test_fold = df_out[ind_test]\n",
    "\n",
    "        #df_train_fold['fold'] = fold\n",
    "        #df_test_fold['fold'] = fold\n",
    "        '''\n",
    "        if counter == 0:\n",
    "            del df_read\n",
    "            gc.collect()\n",
    "            #print(df_train_fold.head())\n",
    "            train = df_train_fold.copy()\n",
    "            del df_train_fold\n",
    "            gc.collect()\n",
    "\n",
    "            valid = df_test_fold.copy()\n",
    "            del df_test_fold\n",
    "            gc.collect()\n",
    "\n",
    "            counter += 1\n",
    "            #print('A')\n",
    "\n",
    "        else:\n",
    "            del df_read\n",
    "            gc.collect()\n",
    "            #print('B')\n",
    "            train = pd.concat([train, df_train_fold]).reset_index(drop=True)\n",
    "            valid = pd.concat([valid, df_test_fold]).reset_index(drop=True)\n",
    "\n",
    "            del df_train_fold, df_test_fold\n",
    "            gc.collect()\n",
    "\n",
    "        #df_train_fold.to_parquet('train_fold_'+str(fold)+'.parquet')\n",
    "        #df_test_fold.to_parquet('test_fold_'+str(fold)+'.parquet')\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c8ae193",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:23:59.612070Z",
     "iopub.status.busy": "2022-01-31T17:23:59.611240Z",
     "iopub.status.idle": "2022-01-31T17:23:59.622397Z",
     "shell.execute_reply": "2022-01-31T17:23:59.623051Z",
     "shell.execute_reply.started": "2022-01-31T15:11:14.646458Z"
    },
    "papermill": {
     "duration": 2.807995,
     "end_time": "2022-01-31T17:23:59.623405",
     "exception": false,
     "start_time": "2022-01-31T17:23:56.815410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Reloading保存後\n",
      "13.1\n",
      "Use Memory total: 2.302899784088135 GiB\n",
      "Peak: 3.969724 GB\n"
     ]
    }
   ],
   "source": [
    "# メモリ使用率を取得\n",
    "mem_bytes = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n",
    "mem = psutil.virtual_memory() \n",
    "print('Train Reloading保存後')\n",
    "print(mem.percent)\n",
    "print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "print('Peak:', peak_memory()/10**6, 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c37ccb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:24:04.852062Z",
     "iopub.status.busy": "2022-01-31T17:24:04.847607Z",
     "iopub.status.idle": "2022-01-31T17:24:07.494197Z",
     "shell.execute_reply": "2022-01-31T17:24:07.493626Z",
     "shell.execute_reply.started": "2022-01-31T15:11:14.655549Z"
    },
    "papermill": {
     "duration": 5.269967,
     "end_time": "2022-01-31T17:24:07.494340",
     "exception": false,
     "start_time": "2022-01-31T17:24:02.224373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2515355, 43)\n",
      "2021-09-21 00:01:00\n",
      "2022-01-23 23:44:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Asset_ID</th>\n",
       "      <th>HperL</th>\n",
       "      <th>log_r</th>\n",
       "      <th>log_ret</th>\n",
       "      <th>realized_vol_15</th>\n",
       "      <th>realized_vol_240</th>\n",
       "      <th>log_ret_15_lag</th>\n",
       "      <th>log_ret_15_std</th>\n",
       "      <th>ENV_up</th>\n",
       "      <th>...</th>\n",
       "      <th>log_ret_lag2</th>\n",
       "      <th>realized_vol_15_lag2</th>\n",
       "      <th>realized_vol_240_lag2</th>\n",
       "      <th>realized_vol_std_sum_15_lag2</th>\n",
       "      <th>realized_vol_std_sum_240_lag2</th>\n",
       "      <th>m15_15mean_lag2</th>\n",
       "      <th>m15_240mean_lag2</th>\n",
       "      <th>m15_15std_lag2</th>\n",
       "      <th>vol240std_lag2</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1632182460</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999770</td>\n",
       "      <td>5.270550e-08</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.024363</td>\n",
       "      <td>0.004875</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.156770</td>\n",
       "      <td>73.849289</td>\n",
       "      <td>-0.000982</td>\n",
       "      <td>0.009697</td>\n",
       "      <td>0.024374</td>\n",
       "      <td>0.124613</td>\n",
       "      <td>1.269582</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>-0.019375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1632182460</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999091</td>\n",
       "      <td>8.271903e-07</td>\n",
       "      <td>-0.000909</td>\n",
       "      <td>0.006673</td>\n",
       "      <td>0.020532</td>\n",
       "      <td>0.003660</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.111814</td>\n",
       "      <td>48.288921</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.020553</td>\n",
       "      <td>0.085028</td>\n",
       "      <td>1.055735</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>-0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1632182460</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999475</td>\n",
       "      <td>2.757085e-07</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>0.008905</td>\n",
       "      <td>0.018004</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.132563</td>\n",
       "      <td>65.391998</td>\n",
       "      <td>-0.001936</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.018004</td>\n",
       "      <td>0.090663</td>\n",
       "      <td>0.871884</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>-0.000607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1632182460</td>\n",
       "      <td>3</td>\n",
       "      <td>0.998764</td>\n",
       "      <td>1.529583e-06</td>\n",
       "      <td>-0.001237</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.033802</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.161315</td>\n",
       "      <td>68.010574</td>\n",
       "      <td>-0.000898</td>\n",
       "      <td>0.012774</td>\n",
       "      <td>0.033779</td>\n",
       "      <td>0.134338</td>\n",
       "      <td>1.794582</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.003085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1632182460</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999396</td>\n",
       "      <td>3.645848e-07</td>\n",
       "      <td>-0.000604</td>\n",
       "      <td>0.011326</td>\n",
       "      <td>0.027561</td>\n",
       "      <td>0.003975</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.091856</td>\n",
       "      <td>30.491564</td>\n",
       "      <td>-0.001575</td>\n",
       "      <td>0.011226</td>\n",
       "      <td>0.027559</td>\n",
       "      <td>0.117603</td>\n",
       "      <td>1.413247</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>-0.001233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  Asset_ID     HperL         log_r   log_ret  realized_vol_15  \\\n",
       "0  1632182460         0  0.999770  5.270550e-08 -0.000230         0.009701   \n",
       "1  1632182460         1  0.999091  8.271903e-07 -0.000909         0.006673   \n",
       "2  1632182460         2  0.999475  2.757085e-07 -0.000525         0.008905   \n",
       "3  1632182460         3  0.998764  1.529583e-06 -0.001237         0.012784   \n",
       "4  1632182460         4  0.999396  3.645848e-07 -0.000604         0.011326   \n",
       "\n",
       "   realized_vol_240  log_ret_15_lag  log_ret_15_std    ENV_up  ...  \\\n",
       "0          0.024363        0.004875        0.002283  0.000935  ...   \n",
       "1          0.020532        0.003660        0.001637  0.001052  ...   \n",
       "2          0.018004        0.002281        0.002211  0.003926  ...   \n",
       "3          0.033802        0.001042        0.003119  0.003789  ...   \n",
       "4          0.027561        0.003975        0.002786  0.000483  ...   \n",
       "\n",
       "   log_ret_lag2  realized_vol_15_lag2  realized_vol_240_lag2  \\\n",
       "0      0.001677              0.156770              73.849289   \n",
       "1      0.001677              0.111814              48.288921   \n",
       "2      0.001677              0.132563              65.391998   \n",
       "3      0.001677              0.161315              68.010574   \n",
       "4      0.001677              0.091856              30.491564   \n",
       "\n",
       "   realized_vol_std_sum_15_lag2  realized_vol_std_sum_240_lag2  \\\n",
       "0                     -0.000982                       0.009697   \n",
       "1                     -0.000173                       0.006693   \n",
       "2                     -0.001936                       0.008693   \n",
       "3                     -0.000898                       0.012774   \n",
       "4                     -0.001575                       0.011226   \n",
       "\n",
       "   m15_15mean_lag2  m15_240mean_lag2  m15_15std_lag2  vol240std_lag2    Target  \n",
       "0         0.024374          0.124613        1.269582        0.000441 -0.019375  \n",
       "1         0.020553          0.085028        1.055735        0.000441 -0.000374  \n",
       "2         0.018004          0.090663        0.871884        0.000441 -0.000607  \n",
       "3         0.033779          0.134338        1.794582        0.000441  0.003085  \n",
       "4         0.027559          0.117603        1.413247        0.000441 -0.001233  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if SUPPLEMENT == True:\n",
    "    \n",
    "    for i in tqdm(range(n_fold)):\n",
    "        if i == 0:\n",
    "            supples = pd.read_parquet(f'./train_fold_{i}.parquet')\n",
    "            print(supples.shape)\n",
    "        else:\n",
    "            base = pd.read_parquet(f'./train_fold_{i}.parquet')\n",
    "            #train = train.append(base)\n",
    "            supples = pd.concat([supples, base], ignore_index=True, copy=False)\n",
    "\n",
    "            # メモリ使用率を取得\n",
    "\n",
    "            mem = psutil.virtual_memory() \n",
    "            print(mem.percent)\n",
    "            print(f'Use Memory total before del: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "            print('-'*100)\n",
    "            #print(train.shape)\n",
    "            print('before:', peak_memory()/10**6, 'GB')\n",
    "\n",
    "            del base \n",
    "            gc.collect()\n",
    "            print('-'*100)\n",
    "            print(mem.percent)\n",
    "            print(f'Use Memory total after del: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "            print('='*100)\n",
    "    \n",
    "    print(pd.to_datetime(supples.head(1)['timestamp'].values[0], unit='s'))\n",
    "    print(pd.to_datetime(supples.tail(1)['timestamp'].values[0], unit='s'))\n",
    "    display(supples.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d38532a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:24:12.892240Z",
     "iopub.status.busy": "2022-01-31T17:24:12.891362Z",
     "iopub.status.idle": "2022-01-31T17:24:40.748638Z",
     "shell.execute_reply": "2022-01-31T17:24:40.749412Z",
     "shell.execute_reply.started": "2022-01-31T15:11:14.955225Z"
    },
    "papermill": {
     "duration": 30.452326,
     "end_time": "2022-01-31T17:24:40.749660",
     "exception": false,
     "start_time": "2022-01-31T17:24:10.297334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "====================================================================================================\n",
      "valid作成前\n",
      "39.1\n",
      "Use Memory total: 6.873540576934815 GiB\n",
      "Peak: 10.66072 GB\n"
     ]
    }
   ],
   "source": [
    "if RETRAIN == True:\n",
    "    for i in tqdm(range(5)):\n",
    "        if i == 0:\n",
    "            train = pd.read_parquet(f'./train_fold_{i}.parquet')\n",
    "            print(train.shape)\n",
    "        else:\n",
    "            base = pd.read_parquet(f'./train_fold_{i}.parquet')\n",
    "            #train = train.append(base)\n",
    "            train = pd.concat([train, base], ignore_index=True, copy=False)\n",
    "\n",
    "            # メモリ使用率を取得\n",
    "\n",
    "            mem = psutil.virtual_memory() \n",
    "            print(mem.percent)\n",
    "            print(f'Use Memory total before del: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "            print('-'*100)\n",
    "            #print(train.shape)\n",
    "            print('before:', peak_memory()/10**6, 'GB')\n",
    "\n",
    "            del base \n",
    "            gc.collect()\n",
    "            print('-'*100)\n",
    "            print(mem.percent)\n",
    "            print(f'Use Memory total after del: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "            print('='*100)\n",
    "\n",
    "    print('='*100)\n",
    "    print('='*100)\n",
    "    filter_validation = pd.to_datetime(train['timestamp'], unit='s') >= '2021-04-01 00:00:00'\n",
    "    filter_train = pd.to_datetime(train['timestamp'], unit='s') < '2021-04-01 00:00:00'\n",
    "\n",
    "    print('='*100)\n",
    "    mem = psutil.virtual_memory() \n",
    "    print('valid作成前')\n",
    "    print(mem.percent)\n",
    "    print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "    print('Peak:', peak_memory()/10**6, 'GB')\n",
    "\n",
    "\n",
    "    valid = train[filter_validation]#\n",
    "    valid.reset_index(drop=True, inplace=True)\n",
    "    #valid = train.copy()#[filter_validation].reset_index(drop=True)\n",
    "    gc.collect()\n",
    "    mem = psutil.virtual_memory() \n",
    "    print('valid作成後')\n",
    "    print(mem.percent)\n",
    "    print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "    print('Peak:', peak_memory()/10**6, 'GB')\n",
    "\n",
    "\n",
    "\n",
    "    train = train[filter_train]\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    mem = psutil.virtual_memory() \n",
    "    print('Train作成gc前')\n",
    "    print(mem.percent)\n",
    "    print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "    print('Peak:', peak_memory()/10**6, 'GB')\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    \n",
    "    train = pd.read_parquet(os.path.join(savepath, '2020_20210920_d2.parquet'))\n",
    "\n",
    "    print('='*100)\n",
    "    filter_validation = pd.to_datetime(train['timestamp'], unit='s') >= '2021-04-01 00:00:00'\n",
    "    filter_train = pd.to_datetime(train['timestamp'], unit='s') < '2021-04-01 00:00:00'\n",
    "\n",
    "    print('='*100)\n",
    "    mem = psutil.virtual_memory() \n",
    "    print('valid作成前')\n",
    "    print(mem.percent)\n",
    "    print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "    print('Peak:', peak_memory()/10**6, 'GB')\n",
    "\n",
    "    \n",
    "    if VALID:\n",
    "        \n",
    "        valid = train[filter_validation]#\n",
    "        valid.reset_index(drop=True, inplace=True)\n",
    "        #valid = train.copy()#[filter_validation].reset_index(drop=True)\n",
    "        gc.collect()\n",
    "        mem = psutil.virtual_memory() \n",
    "        print('valid作成後')\n",
    "        print(mem.percent)\n",
    "        print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "        print('Peak:', peak_memory()/10**6, 'GB')\n",
    "\n",
    "\n",
    "        train = train[filter_train]\n",
    "        train.reset_index(drop=True, inplace=True)\n",
    "        mem = psutil.virtual_memory() \n",
    "        print('Train作成gc前')\n",
    "        print(mem.percent)\n",
    "        print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "        print('Peak:', peak_memory()/10**6, 'GB')\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        filter_leakage = pd.to_datetime(train['timestamp'], unit='s') < '2021-06-13 00:00:00'\n",
    "        filter_memory = pd.to_datetime(train['timestamp'], unit='s') >= '2019-01-01 00:00:00'\n",
    "\n",
    "        #if LEAKAGE_FILTER:\n",
    "        #    train = train[filter_leakage]\n",
    "\n",
    "        if YEAR_FILTER:\n",
    "            train = train[filter_memory]\n",
    "            train.reset_index(drop=True, inplace=True)\n",
    "            print('Train作成gc前')\n",
    "            print(mem.percent)\n",
    "            print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "            print('Peak:', peak_memory()/10**6, 'GB')\n",
    "            gc.collect()\n",
    "    \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1cfd20c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:24:46.012322Z",
     "iopub.status.busy": "2022-01-31T17:24:46.011303Z",
     "iopub.status.idle": "2022-01-31T17:24:46.014781Z",
     "shell.execute_reply": "2022-01-31T17:24:46.014285Z",
     "shell.execute_reply.started": "2022-01-31T15:11:47.567949Z"
    },
    "papermill": {
     "duration": 2.660934,
     "end_time": "2022-01-31T17:24:46.014939",
     "exception": false,
     "start_time": "2022-01-31T17:24:43.354005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 00:01:00\n",
      "2021-09-20 23:44:00\n"
     ]
    }
   ],
   "source": [
    "print(pd.to_datetime(train.head(1)['timestamp'].values[0], unit='s'))\n",
    "print(pd.to_datetime(train.tail(1)['timestamp'].values[0], unit='s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61dabeaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:24:51.144203Z",
     "iopub.status.busy": "2022-01-31T17:24:51.143383Z",
     "iopub.status.idle": "2022-01-31T17:25:05.029929Z",
     "shell.execute_reply": "2022-01-31T17:25:05.030469Z",
     "shell.execute_reply.started": "2022-01-31T15:11:47.579575Z"
    },
    "papermill": {
     "duration": 16.446483,
     "end_time": "2022-01-31T17:25:05.030677",
     "exception": false,
     "start_time": "2022-01-31T17:24:48.584194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11966420, 43)\n",
      "(6781327, 43)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "if RESAMPLE:\n",
    "    train = train.sample(frac=0.59, random_state=91, ignore_index=True)\n",
    "    train.sort_values('timestamp', inplace=True)\n",
    "    \n",
    "    delcount = len(supples) - basesize\n",
    "    \n",
    "    train = train.iloc[delcount:, :]\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3927fa84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:25:10.213343Z",
     "iopub.status.busy": "2022-01-31T17:25:10.212596Z",
     "iopub.status.idle": "2022-01-31T17:25:12.716889Z",
     "shell.execute_reply": "2022-01-31T17:25:12.716108Z",
     "shell.execute_reply.started": "2022-01-31T15:12:00.69478Z"
    },
    "papermill": {
     "duration": 5.062523,
     "end_time": "2022-01-31T17:25:12.717038",
     "exception": false,
     "start_time": "2022-01-31T17:25:07.654515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9296682, 43)\n"
     ]
    }
   ],
   "source": [
    "# SUPPLEと結合\n",
    "if SUPPLEMENT == True:\n",
    "    train = pd.concat([train, supples], ignore_index=True, copy=False)\n",
    "    del supples\n",
    "    gc.collect()\n",
    "    print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b03644c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:25:18.010998Z",
     "iopub.status.busy": "2022-01-31T17:25:18.010158Z",
     "iopub.status.idle": "2022-01-31T17:25:18.015269Z",
     "shell.execute_reply": "2022-01-31T17:25:18.015810Z",
     "shell.execute_reply.started": "2022-01-31T15:12:01.310609Z"
    },
    "papermill": {
     "duration": 2.636379,
     "end_time": "2022-01-31T17:25:18.016022",
     "exception": false,
     "start_time": "2022-01-31T17:25:15.379643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Reloadingあと\n",
      "30.0\n",
      "Use Memory total: 5.273816299438477 GiB\n",
      "Peak: 10.66072 GB\n"
     ]
    }
   ],
   "source": [
    "# メモリ使用率を取得\n",
    "mem = psutil.virtual_memory() \n",
    "print('Train Reloadingあと')\n",
    "print(mem.percent)\n",
    "print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "print('Peak:', peak_memory()/10**6, 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30495b20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:25:23.264763Z",
     "iopub.status.busy": "2022-01-31T17:25:23.264016Z",
     "iopub.status.idle": "2022-01-31T17:25:23.303307Z",
     "shell.execute_reply": "2022-01-31T17:25:23.303813Z",
     "shell.execute_reply.started": "2022-01-31T15:12:01.320292Z"
    },
    "papermill": {
     "duration": 2.676753,
     "end_time": "2022-01-31T17:25:23.304005",
     "exception": false,
     "start_time": "2022-01-31T17:25:20.627252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Asset_ID</th>\n",
       "      <th>HperL</th>\n",
       "      <th>log_r</th>\n",
       "      <th>log_ret</th>\n",
       "      <th>realized_vol_15</th>\n",
       "      <th>realized_vol_240</th>\n",
       "      <th>log_ret_15_lag</th>\n",
       "      <th>log_ret_15_std</th>\n",
       "      <th>ENV_up</th>\n",
       "      <th>...</th>\n",
       "      <th>log_ret_lag2</th>\n",
       "      <th>realized_vol_15_lag2</th>\n",
       "      <th>realized_vol_240_lag2</th>\n",
       "      <th>realized_vol_std_sum_15_lag2</th>\n",
       "      <th>realized_vol_std_sum_240_lag2</th>\n",
       "      <th>m15_15mean_lag2</th>\n",
       "      <th>m15_240mean_lag2</th>\n",
       "      <th>m15_15std_lag2</th>\n",
       "      <th>vol240std_lag2</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1580206980</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000477</td>\n",
       "      <td>2.273363e-07</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>-0.001096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.143691</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.006339</td>\n",
       "      <td>0.012756</td>\n",
       "      <td>0.368343</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.005791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1580207040</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000488</td>\n",
       "      <td>2.383707e-07</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>0.013493</td>\n",
       "      <td>0.009269</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>-0.001576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.146778</td>\n",
       "      <td>35.355247</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.004609</td>\n",
       "      <td>0.013489</td>\n",
       "      <td>0.067099</td>\n",
       "      <td>0.740592</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.006807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1580207040</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000418</td>\n",
       "      <td>1.747403e-07</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.006365</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>-0.001339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.143763</td>\n",
       "      <td>45.968586</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>0.012842</td>\n",
       "      <td>0.366611</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.004799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1580207040</td>\n",
       "      <td>3</td>\n",
       "      <td>1.002951</td>\n",
       "      <td>8.684016e-06</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.005945</td>\n",
       "      <td>0.037642</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.166585</td>\n",
       "      <td>40.416668</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.005677</td>\n",
       "      <td>0.037527</td>\n",
       "      <td>0.078178</td>\n",
       "      <td>1.853841</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.001504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1580207040</td>\n",
       "      <td>9</td>\n",
       "      <td>1.001632</td>\n",
       "      <td>2.659954e-06</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.003315</td>\n",
       "      <td>0.010048</td>\n",
       "      <td>0.009525</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>-0.002839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.114858</td>\n",
       "      <td>43.766735</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.003315</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>0.038019</td>\n",
       "      <td>0.547132</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.003708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  Asset_ID     HperL         log_r   log_ret  realized_vol_15  \\\n",
       "0  1580206980        12  1.000477  2.273363e-07  0.000477         0.001020   \n",
       "1  1580207040        13  1.000488  2.383707e-07  0.000488         0.004702   \n",
       "2  1580207040        12  1.000418  1.747403e-07  0.000418         0.001123   \n",
       "3  1580207040         3  1.002951  8.684016e-06  0.002947         0.005945   \n",
       "4  1580207040         9  1.001632  2.659954e-06  0.001631         0.003315   \n",
       "\n",
       "   realized_vol_240  log_ret_15_lag  log_ret_15_std    ENV_up  ...  \\\n",
       "0          0.006357        0.001847        0.000276 -0.001096  ...   \n",
       "1          0.013493        0.009269        0.001003 -0.001576  ...   \n",
       "2          0.006365        0.002424        0.000287 -0.001339  ...   \n",
       "3          0.037642        0.001980        0.001695  0.002283  ...   \n",
       "4          0.010048        0.009525        0.000797 -0.002839  ...   \n",
       "\n",
       "   log_ret_lag2  realized_vol_15_lag2  realized_vol_240_lag2  \\\n",
       "0      0.000368              0.143691              46.000000   \n",
       "1      0.000447              0.146778              35.355247   \n",
       "2      0.000447              0.143763              45.968586   \n",
       "3      0.000447              0.166585              40.416668   \n",
       "4      0.000447              0.114858              43.766735   \n",
       "\n",
       "   realized_vol_std_sum_15_lag2  realized_vol_std_sum_240_lag2  \\\n",
       "0                     -0.000075                       0.001041   \n",
       "1                      0.001112                       0.004609   \n",
       "2                      0.000477                       0.001020   \n",
       "3                      0.001788                       0.005677   \n",
       "4                      0.000112                       0.003315   \n",
       "\n",
       "   m15_15mean_lag2  m15_240mean_lag2  m15_15std_lag2  vol240std_lag2    Target  \n",
       "0         0.006339          0.012756        0.368343        0.000269  0.005791  \n",
       "1         0.013489          0.067099        0.740592        0.000300  0.006807  \n",
       "2         0.006357          0.012842        0.366611        0.000300  0.004799  \n",
       "3         0.037527          0.078178        1.853841        0.000300  0.001504  \n",
       "4         0.009917          0.038019        0.547132        0.000300  0.003708  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Asset_ID</th>\n",
       "      <th>HperL</th>\n",
       "      <th>log_r</th>\n",
       "      <th>log_ret</th>\n",
       "      <th>realized_vol_15</th>\n",
       "      <th>realized_vol_240</th>\n",
       "      <th>log_ret_15_lag</th>\n",
       "      <th>log_ret_15_std</th>\n",
       "      <th>ENV_up</th>\n",
       "      <th>...</th>\n",
       "      <th>log_ret_lag2</th>\n",
       "      <th>realized_vol_15_lag2</th>\n",
       "      <th>realized_vol_240_lag2</th>\n",
       "      <th>realized_vol_std_sum_15_lag2</th>\n",
       "      <th>realized_vol_std_sum_240_lag2</th>\n",
       "      <th>m15_15mean_lag2</th>\n",
       "      <th>m15_240mean_lag2</th>\n",
       "      <th>m15_15std_lag2</th>\n",
       "      <th>vol240std_lag2</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9296677</th>\n",
       "      <td>1642981440</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000289</td>\n",
       "      <td>8.367403e-08</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>0.028407</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>-0.000827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.107135</td>\n",
       "      <td>18.330618</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.028412</td>\n",
       "      <td>0.101516</td>\n",
       "      <td>1.472337</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9296678</th>\n",
       "      <td>1642981440</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000647</td>\n",
       "      <td>4.182542e-07</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>0.023972</td>\n",
       "      <td>-0.002785</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.111720</td>\n",
       "      <td>24.812679</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>0.023963</td>\n",
       "      <td>0.066806</td>\n",
       "      <td>1.140551</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.002006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9296679</th>\n",
       "      <td>1642981440</td>\n",
       "      <td>11</td>\n",
       "      <td>0.999680</td>\n",
       "      <td>1.025337e-07</td>\n",
       "      <td>-0.000320</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>0.025811</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>-0.000911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.117991</td>\n",
       "      <td>15.304606</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.007363</td>\n",
       "      <td>0.025817</td>\n",
       "      <td>0.102468</td>\n",
       "      <td>1.314910</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>0.000406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9296680</th>\n",
       "      <td>1642981440</td>\n",
       "      <td>12</td>\n",
       "      <td>1.001190</td>\n",
       "      <td>1.413690e-06</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.091844</td>\n",
       "      <td>26.067522</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.027074</td>\n",
       "      <td>0.116046</td>\n",
       "      <td>1.383625</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.001046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9296681</th>\n",
       "      <td>1642981440</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>5.532951e-08</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.006476</td>\n",
       "      <td>0.022716</td>\n",
       "      <td>-0.004386</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.003555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.076103</td>\n",
       "      <td>10.416759</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.022714</td>\n",
       "      <td>0.082288</td>\n",
       "      <td>1.065773</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>0.000281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          timestamp  Asset_ID     HperL         log_r   log_ret  \\\n",
       "9296677  1642981440         9  1.000289  8.367403e-08  0.000289   \n",
       "9296678  1642981440        10  1.000647  4.182542e-07  0.000647   \n",
       "9296679  1642981440        11  0.999680  1.025337e-07 -0.000320   \n",
       "9296680  1642981440        12  1.001190  1.413690e-06  0.001189   \n",
       "9296681  1642981440        13  1.000235  5.532951e-08  0.000235   \n",
       "\n",
       "         realized_vol_15  realized_vol_240  log_ret_15_lag  log_ret_15_std  \\\n",
       "9296677         0.005176          0.028407        0.003167        0.001252   \n",
       "9296678         0.003105          0.023972       -0.002785        0.000798   \n",
       "9296679         0.006513          0.025811        0.008499        0.001631   \n",
       "9296680         0.005232          0.027100        0.004869        0.001336   \n",
       "9296681         0.006476          0.022716       -0.004386        0.001632   \n",
       "\n",
       "           ENV_up  ...  log_ret_lag2  realized_vol_15_lag2  \\\n",
       "9296677 -0.000827  ...      0.001378              0.107135   \n",
       "9296678  0.003547  ...      0.001378              0.111720   \n",
       "9296679 -0.000911  ...      0.001378              0.117991   \n",
       "9296680 -0.000416  ...      0.001378              0.091844   \n",
       "9296681  0.003555  ...      0.001378              0.076103   \n",
       "\n",
       "         realized_vol_240_lag2  realized_vol_std_sum_15_lag2  \\\n",
       "9296677              18.330618                      0.002649   \n",
       "9296678              24.812679                      0.000285   \n",
       "9296679              15.304606                      0.000948   \n",
       "9296680              26.067522                      0.001647   \n",
       "9296681              10.416759                      0.003125   \n",
       "\n",
       "         realized_vol_std_sum_240_lag2  m15_15mean_lag2  m15_240mean_lag2  \\\n",
       "9296677                       0.005421         0.028412          0.101516   \n",
       "9296678                       0.003752         0.023963          0.066806   \n",
       "9296679                       0.007363         0.025817          0.102468   \n",
       "9296680                       0.007514         0.027074          0.116046   \n",
       "9296681                       0.006324         0.022714          0.082288   \n",
       "\n",
       "         m15_15std_lag2  vol240std_lag2    Target  \n",
       "9296677        1.472337       -0.000466  0.000047  \n",
       "9296678        1.140551       -0.000466 -0.002006  \n",
       "9296679        1.314910       -0.000466  0.000406  \n",
       "9296680        1.383625       -0.000466 -0.001046  \n",
       "9296681        1.065773       -0.000466  0.000281  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-28 10:23:00\n",
      "2022-01-23 23:44:00\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "display(train.head())\n",
    "print('='*100)\n",
    "display(train.tail())\n",
    "\n",
    "print(pd.to_datetime(train.head(1)['timestamp'].values[0], unit='s'))\n",
    "print(pd.to_datetime(train.tail(1)['timestamp'].values[0], unit='s'))\n",
    "print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73b01c44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:25:28.433543Z",
     "iopub.status.busy": "2022-01-31T17:25:28.432724Z",
     "iopub.status.idle": "2022-01-31T17:25:28.434220Z",
     "shell.execute_reply": "2022-01-31T17:25:28.434710Z",
     "shell.execute_reply.started": "2022-01-31T15:12:01.382725Z"
    },
    "papermill": {
     "duration": 2.571204,
     "end_time": "2022-01-31T17:25:28.434900",
     "exception": false,
     "start_time": "2022-01-31T17:25:25.863696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if VALID:\n",
    "    display(valid.head())\n",
    "    print('='*100)\n",
    "    display(valid.tail())\n",
    "\n",
    "    print(pd.to_datetime(valid.head(1)['timestamp'].values[0], unit='s'))\n",
    "    print(pd.to_datetime(valid.tail(1)['timestamp'].values[0], unit='s'))\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "291be418",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:25:33.658504Z",
     "iopub.status.busy": "2022-01-31T17:25:33.657782Z",
     "iopub.status.idle": "2022-01-31T17:25:33.663103Z",
     "shell.execute_reply": "2022-01-31T17:25:33.663672Z",
     "shell.execute_reply.started": "2022-01-31T15:12:01.391999Z"
    },
    "papermill": {
     "duration": 2.569833,
     "end_time": "2022-01-31T17:25:33.663843",
     "exception": false,
     "start_time": "2022-01-31T17:25:31.094010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asset_ID', 'HperL', 'log_r', 'log_ret', 'realized_vol_15', 'realized_vol_240', 'log_ret_15_lag', 'log_ret_15_std', 'ENV_up', 'ENV_do', 'RSI15', 'RSI240', 'macd60', 'signal15', 'signal60', 'realized_vol_std_sum_15', 'realized_vol_std_sum_240', 'm15_15mean', 'm15_240mean', 'm15_15std', 'vol240std', 'c240sum', 'log_ret_lag1', 'realized_vol_15_lag1', 'realized_vol_240_lag1', 'realized_vol_std_sum_15_lag1', 'realized_vol_std_sum_240_lag1', 'm15_15mean_lag1', 'm15_240mean_lag1', 'm15_15std_lag1', 'vol240std_lag1', 'c240sum_lag1', 'log_ret_lag2', 'realized_vol_15_lag2', 'realized_vol_240_lag2', 'realized_vol_std_sum_15_lag2', 'realized_vol_std_sum_240_lag2', 'm15_15mean_lag2', 'm15_240mean_lag2', 'm15_15std_lag2', 'vol240std_lag2']\n",
      "====================================================================================================\n",
      "['log_r', 'log_ret', 'realized_vol_15', 'realized_vol_240', 'log_ret_15_lag', 'ENV_up', 'ENV_do', 'm15_15mean', 'm15_240mean', 'm15_15std', 'log_ret_lag1', 'realized_vol_15_lag1', 'realized_vol_240_lag1', 'realized_vol_std_sum_15_lag1', 'realized_vol_std_sum_240_lag1', 'm15_15mean_lag1', 'm15_240mean_lag1', 'm15_15std_lag1', 'vol240std_lag1', 'log_ret_lag2', 'realized_vol_15_lag2', 'realized_vol_240_lag2', 'realized_vol_std_sum_15_lag2', 'realized_vol_std_sum_240_lag2', 'm15_15mean_lag2', 'm15_240mean_lag2', 'm15_15std_lag2', 'vol240std_lag2']\n"
     ]
    }
   ],
   "source": [
    "features = colabcols\n",
    "print(features)\n",
    "print(\"=\"*100)\n",
    "features_ridge = colabcols_ridge\n",
    "print(features_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb8d6e8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:25:38.776199Z",
     "iopub.status.busy": "2022-01-31T17:25:38.775467Z",
     "iopub.status.idle": "2022-01-31T17:25:38.781764Z",
     "shell.execute_reply": "2022-01-31T17:25:38.782390Z",
     "shell.execute_reply.started": "2022-01-31T15:12:01.408395Z"
    },
    "papermill": {
     "duration": 2.565418,
     "end_time": "2022-01-31T17:25:38.782580",
     "exception": false,
     "start_time": "2022-01-31T17:25:36.217162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainm前\n",
      "30.0\n",
      "Use Memory total: 5.273816299438477 GiB\n",
      "Peak: 10.66072 GB\n"
     ]
    }
   ],
   "source": [
    "# メモリ使用率を取得\n",
    "mem = psutil.virtual_memory() \n",
    "print('Trainm前')\n",
    "print(mem.percent)\n",
    "print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "print('Peak:', peak_memory()/10**6, 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cace08c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:25:43.992080Z",
     "iopub.status.busy": "2022-01-31T17:25:43.991359Z",
     "iopub.status.idle": "2022-01-31T17:25:44.007254Z",
     "shell.execute_reply": "2022-01-31T17:25:44.007795Z",
     "shell.execute_reply.started": "2022-01-31T15:12:01.424986Z"
    },
    "papermill": {
     "duration": 2.582229,
     "end_time": "2022-01-31T17:25:44.008008",
     "exception": false,
     "start_time": "2022-01-31T17:25:41.425779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_correlation(a, b, weights):\n",
    "\n",
    "  '''\n",
    "  a : pred\n",
    "  b : target\n",
    "  c : weight\n",
    "  '''  \n",
    "  w = np.ravel(weights)\n",
    "  a = np.ravel(a)\n",
    "  b = np.ravel(b)\n",
    "\n",
    "  sum_w = np.sum(w)\n",
    "  mean_a = np.sum(a * w) / sum_w\n",
    "  mean_b = np.sum(b * w) / sum_w\n",
    "  var_a = np.sum(w * np.square(a - mean_a)) / sum_w\n",
    "  var_b = np.sum(w * np.square(b - mean_b)) / sum_w\n",
    "\n",
    "  cov = np.sum((a * b * w)) / np.sum(w) - mean_a * mean_b\n",
    "  corr = cov / np.sqrt(var_a * var_b)\n",
    "\n",
    "  return corr\n",
    "\n",
    "##metric\n",
    "# https://stackoverflow.com/questions/38641691/weighted-correlation-coefficient-with-pandas\n",
    "def wmean(x, w):\n",
    "    return np.sum(x * w) / np.sum(w)\n",
    "\n",
    "def wcov(x, y, w):\n",
    "    return np.sum(w * (x - wmean(x, w)) * (y - wmean(y, w))) / np.sum(w)\n",
    "\n",
    "def wcorr(x, y, w):\n",
    "    return wcov(x, y, w) / np.sqrt(wcov(x, x, w) * wcov(y, y, w))\n",
    "\n",
    "def eval_wcorr(preds, train_data):\n",
    "    w = train_data.add_w.values.flatten()\n",
    "    y_true = train_data.get_label()\n",
    "    return 'eval_wcorr', wcorr(preds, y_true, w), True\n",
    "\n",
    "# vizualize\n",
    "def visualize_importance(models, feat_train_df):\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for i, model in enumerate(models):\n",
    "        _df = pd.DataFrame()\n",
    "        _df['feature_importance'] = model.feature_importance(importance_type=\"gain\")\n",
    "        _df['column'] = feat_train_df.columns\n",
    "        _df['fold'] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n",
    "\n",
    "    order = feature_importance_df.groupby('column')\\\n",
    "        .sum()[['feature_importance']]\\\n",
    "        .sort_values('feature_importance', ascending=False).index[:50]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(len(order) * .4, 7))\n",
    "    sns.boxenplot(data=feature_importance_df, x='column', y='feature_importance', order=order, ax=ax, palette='viridis')\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    fig.tight_layout()\n",
    "    return fig, ax, feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7b64245",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:25:49.284652Z",
     "iopub.status.busy": "2022-01-31T17:25:49.279058Z",
     "iopub.status.idle": "2022-01-31T17:40:34.648333Z",
     "shell.execute_reply": "2022-01-31T17:40:34.649292Z",
     "shell.execute_reply.started": "2022-01-31T15:12:01.443284Z"
    },
    "papermill": {
     "duration": 888.033803,
     "end_time": "2022-01-31T17:40:34.649640",
     "exception": false,
     "start_time": "2022-01-31T17:25:46.615837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainm前\n",
      "45.8\n",
      "Use Memory total: 8.051359550476073 GiB\n",
      "Peak: 10.66072 GB\n",
      "====================================================================================================\n",
      "A\n",
      "Trainm前\n",
      "45.8\n",
      "Use Memory total: 8.051359550476073 GiB\n",
      "Peak: 10.66072 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "ASSET :  0\n",
      "ASSET :  1\n",
      "ASSET :  2\n",
      "ASSET :  3\n",
      "ASSET :  4\n",
      "ASSET :  5\n",
      "ASSET :  6\n",
      "ASSET :  7\n",
      "ASSET :  8\n",
      "ASSET :  9\n",
      "ASSET :  10\n",
      "ASSET :  11\n",
      "ASSET :  12\n",
      "ASSET :  13\n"
     ]
    }
   ],
   "source": [
    "#features = [col for col in train.columns if col not in {'timestamp', 'Target', 'Target_M','weights', 'fold', 'rang'}]\n",
    "\n",
    "\n",
    "#valid_oof = np.zeros((valid.shape[0],))\n",
    "#valid_oof2 = np.zeros((valid.shape[0],))\n",
    "#valid_oof3= np.zeros((valid.shape[0],))\n",
    "\n",
    "models = []\n",
    "models2 = []\n",
    "models3 = []\n",
    "models4 = []\n",
    "\n",
    "n_fold = 1\n",
    "\n",
    "if VALID:\n",
    "    timest = valid['timestamp']\n",
    "    weights_tr = pd.merge(train[['Asset_ID']], asset_details[['Asset_ID', 'Weight']], on='Asset_ID', how='left')['Weight']\n",
    "    weights_val = pd.merge(valid[['Asset_ID']], asset_details[['Asset_ID', 'Weight']], on='Asset_ID', how='left')['Weight']\n",
    "    \n",
    "    for fold in range(n_fold):\n",
    "        evals_result = {}\n",
    "        #print(fold)\n",
    "        ind_t = train.index\n",
    "        y_train = train['Target'].values# * 1000 \n",
    "        train = train[features]\n",
    "        train.fillna(-999, inplace=True)\n",
    "        train.replace(np.inf, -999, inplace=True)\n",
    "\n",
    "        #y_train.fillna(-999, inplace=True)\n",
    "        #y_train.replace(np.inf, -999, inplace=True)\n",
    "\n",
    "        train['Asset_ID'] = train['Asset_ID'].astype('category')\n",
    "\n",
    "        ind_v = valid.index\n",
    "        y_val = valid['Target'].values #* 1000\n",
    "        valid = valid[features]\n",
    "        valid.fillna(-999, inplace=True)\n",
    "        valid.replace(np.inf, -999, inplace=True)\n",
    "\n",
    "\n",
    "        valid['Asset_ID'] = valid['Asset_ID'].astype('category')\n",
    "\n",
    "        #weights_tr_fold = weights_tr[ind_t].reset_index(drop=True, inplace=True)\n",
    "        #weights_val_fold = weights_val[ind_v].reset_index(drop=True, inplace=True)\n",
    "        mem = psutil.virtual_memory() \n",
    "        print('Trainm前')\n",
    "        print(mem.percent)\n",
    "        print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "        print('Peak:', peak_memory()/10**6, 'GB')\n",
    "        print(\"=\"*100)\n",
    "\n",
    "\n",
    "        train_dataset = lgb.Dataset(train, y_train, feature_name = features, categorical_feature= ['Asset_ID'])\n",
    "        val_dataset = lgb.Dataset(valid, y_val, feature_name = features, categorical_feature= ['Asset_ID'])\n",
    "        #train_dataset.add_w = weights_tr_fold\n",
    "        #val_dataset.add_w = weights_val_fold\n",
    "        train_dataset.add_w = weights_tr\n",
    "        val_dataset.add_w = weights_val\n",
    "        #weights_train = train[['weights']]\n",
    "        #weights_test = test[['weights']]\n",
    "\n",
    "        #scaler = StandardScaler()\n",
    "        #scaler.fit(x_train)\n",
    "\n",
    "        #x_train = scaler.transform(x_train)\n",
    "        #print(x_train.shape)\n",
    "        print('A')\n",
    "        # メモリ使用率を取得\n",
    "        mem = psutil.virtual_memory() \n",
    "        print('Trainm前')\n",
    "        print(mem.percent)\n",
    "        print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "        print('Peak:', peak_memory()/10**6, 'GB')\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',  #objectives = ['regression','regression_l1', 'huber', 'fair','quantile', 'mape', 'gamma','tweedie']\n",
    "            #'fair_c': 100,\n",
    "            #'metric': 'None',\n",
    "            'boosting_type': 'gbdt',\n",
    "            #'max_bin':255,\n",
    "            'min_data_in_leaf':150,\n",
    "            'subsample': 0.7,\n",
    "            'subsample_freq': 5, # 何回に一回バギングするか\n",
    "            #'feature_fraction': 0.4,\n",
    "            'max_depth': -1,\n",
    "            #'num_leaves':32, \n",
    "            'learning_rate': 0.05,\n",
    "            #'subsample_freq': 4,\n",
    "            #'feature_fraction': 0.4,\n",
    "            'colsample_bytree': 0.9, \n",
    "            'lambda_l1': 10,\n",
    "            'lambda_l2': 10,\n",
    "            'seed': 1991,\n",
    "            'verbose': -1,\n",
    "            'n_jobs':-1\n",
    "            }\n",
    "\n",
    "        model = lgb.train(params = params,\n",
    "                              num_boost_round = 300, \n",
    "                              train_set = train_dataset, \n",
    "                              valid_sets = [val_dataset],\n",
    "                              #early_stopping_rounds=20,\n",
    "                              verbose_eval = 50,\n",
    "                              feval=eval_wcorr,\n",
    "                              evals_result = evals_result \n",
    "                             )\n",
    "\n",
    "        #model = Ridge(alpha=5)\n",
    "        #model.fit(x_train, y_train)\n",
    "\n",
    "        #x_val = scaler.transform(x_val)\n",
    "        val_pred = model.predict(valid)\n",
    "        #valid_oof[ind_v] = val_pred\n",
    "        score = weighted_correlation(val_pred, y_val, weights_val)\n",
    "        models.append(model)\n",
    "        print(\"Fold metric score:\", score)\n",
    "        print(\"Fold single corr:\", np.corrcoef(val_pred, y_val)[0][1])\n",
    "        print(pearsonr(y_val, val_pred))\n",
    "        print(\"=\"*100)\n",
    "\n",
    "        del train_dataset\n",
    "        del val_dataset\n",
    "        gc.collect()\n",
    "\n",
    "        train_dataset = lgb.Dataset(train, y_train, feature_name = features, categorical_feature= ['Asset_ID'])\n",
    "        val_dataset = lgb.Dataset(valid, y_val, feature_name = features, categorical_feature= ['Asset_ID'])\n",
    "        train_dataset.add_w = weights_tr\n",
    "        val_dataset.add_w = weights_val\n",
    "\n",
    "        params2 = {\n",
    "            'objective': 'regression',  #objectives = ['regression','regression_l1', 'huber', 'fair','quantile', 'mape', 'gamma','tweedie']\n",
    "            #'fair_c': 100,\n",
    "            #'metric': 'None',\n",
    "            'boosting_type': 'gbdt',\n",
    "            #'max_bin':255,\n",
    "            'min_data_in_leaf':150,\n",
    "            'subsample': 0.7,\n",
    "            'subsample_freq': 5, # 何回に一回バギングするか\n",
    "            #'feature_fraction': 0.4,\n",
    "            'max_depth': -1,\n",
    "            #'num_leaves':32, \n",
    "            'learning_rate': 0.05,\n",
    "            #'subsample_freq': 4,\n",
    "            #'feature_fraction': 0.4,\n",
    "            'colsample_bytree': 0.9, \n",
    "            'lambda_l1': 10,\n",
    "            'lambda_l2': 10,\n",
    "            'seed': 91,\n",
    "            'verbose': -1,\n",
    "            'n_jobs':-1\n",
    "            }\n",
    "\n",
    "        model = lgb.train(params = params2,\n",
    "                              num_boost_round = 300, \n",
    "                              train_set = train_dataset, \n",
    "                              valid_sets = [val_dataset],\n",
    "                              #early_stopping_rounds=20,\n",
    "                              verbose_eval = 50,\n",
    "                              feval=eval_wcorr,\n",
    "                              evals_result = evals_result \n",
    "                             )\n",
    "\n",
    "        #model = Ridge(alpha=5)\n",
    "        #model.fit(x_train, y_train)\n",
    "\n",
    "        #x_val = scaler.transform(x_val)\n",
    "        val_pred2 = model.predict(valid)\n",
    "        #valid_oof2[ind_v] = val_pred2\n",
    "        score = weighted_correlation(val_pred2, y_val, weights_val)\n",
    "        models2.append(model)\n",
    "        print(\"Fold metric score:\", score)\n",
    "        print(\"Fold single corr:\", np.corrcoef(val_pred2, y_val)[0][1])\n",
    "        print(pearsonr(y_val, val_pred2))\n",
    "        print(\"=\"*100)\n",
    "\n",
    "\n",
    "        del train_dataset\n",
    "        del val_dataset\n",
    "        gc.collect()\n",
    "\n",
    "        train_dataset = lgb.Dataset(train, y_train, feature_name = features, categorical_feature= ['Asset_ID'])\n",
    "        val_dataset = lgb.Dataset(valid, y_val, feature_name = features, categorical_feature= ['Asset_ID'])\n",
    "        train_dataset.add_w = weights_tr\n",
    "        val_dataset.add_w = weights_val\n",
    "\n",
    "        params3 = {\n",
    "            'objective': 'regression',  #objectives = ['regression','regression_l1', 'huber', 'fair','quantile', 'mape', 'gamma','tweedie']\n",
    "            #'fair_c': 100,\n",
    "            #'metric': 'None',\n",
    "            'boosting_type': 'gbdt',\n",
    "            #'max_bin':255,\n",
    "            'min_data_in_leaf':150,\n",
    "            'subsample': 0.7,\n",
    "            'subsample_freq': 5, # 何回に一回バギングするか\n",
    "            #'feature_fraction': 0.4,\n",
    "            'max_depth': -1,\n",
    "            #'num_leaves':32, \n",
    "            'learning_rate': 0.051,\n",
    "            #'subsample_freq': 4,\n",
    "            #'feature_fraction': 0.4,\n",
    "            'colsample_bytree': 0.9, \n",
    "            'lambda_l1': 10,\n",
    "            'lambda_l2': 10,\n",
    "            'seed': 31,\n",
    "            'verbose': -1,\n",
    "            'n_jobs':-1\n",
    "            }\n",
    "\n",
    "        model = lgb.train(params = params3,\n",
    "                              num_boost_round = 300, \n",
    "                              train_set = train_dataset, \n",
    "                              valid_sets = [val_dataset],\n",
    "                              #early_stopping_rounds=20,\n",
    "                              verbose_eval = 50,\n",
    "                              feval=eval_wcorr,\n",
    "                              evals_result = evals_result \n",
    "                             )\n",
    "\n",
    "        #model = Ridge(alpha=5)\n",
    "        #model.fit(x_train, y_train)\n",
    "\n",
    "        #x_val = scaler.transform(x_val)\n",
    "        val_pred3 = model.predict(valid)\n",
    "        #valid_oof3[ind_v] = val_pred3\n",
    "        score = weighted_correlation(val_pred3, y_val, weights_val)\n",
    "        models3.append(model)\n",
    "        print(\"Fold metric score:\", score)\n",
    "        print(\"Fold single corr:\", np.corrcoef(val_pred3, y_val)[0][1])\n",
    "        print(pearsonr(y_val, val_pred3))\n",
    "        print(\"=\"*100)\n",
    "\n",
    "        del train_dataset\n",
    "        del val_dataset\n",
    "        gc.collect()\n",
    "\n",
    "        ################################# ridge #################################\n",
    "        \n",
    "        model4 = Ridge(alpha=10)\n",
    "        # model = lgb.LGBMRegressor(n_jobs=-1, random_state=1)\n",
    "\n",
    "        model4 = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model4)\n",
    "        ])\n",
    "\n",
    "        model4.fit(train[features_ridge].values, y_train)\n",
    "\n",
    "        #model = Ridge(alpha=5)\n",
    "        #model.fit(x_train, y_train)\n",
    "\n",
    "        #x_val = scaler.transform(x_val)\n",
    "        val_pred4 = model4.predict(valid[features_ridge].values)\n",
    "        #valid_oof[ind_v] = val_pred\n",
    "        score = weighted_correlation(val_pred4, y_val, weights_val)\n",
    "        models4.append(model4)\n",
    "        print(\"Fold metric score:\", score)\n",
    "        print(\"Fold single corr:\", np.corrcoef(val_pred4, y_val)[0][1])\n",
    "        print(pearsonr(y_val, val_pred4))\n",
    "        print(\"=\"*100)\n",
    "\n",
    "else:\n",
    "    weights_tr = pd.merge(train[['Asset_ID']], asset_details[['Asset_ID', 'Weight']], on='Asset_ID', how='left')['Weight']\n",
    "    #weights_val = pd.merge(valid[['Asset_ID']], asset_details[['Asset_ID', 'Weight']], on='Asset_ID', how='left')['Weight']\n",
    "    for fold in range(n_fold):\n",
    "        evals_result = {}\n",
    "        #print(fold)\n",
    "        ind_t = train.index\n",
    "        y_train = train['Target'].values# * 1000 \n",
    "        train = train[features]\n",
    "        train.fillna(-999, inplace=True)\n",
    "        train.replace(np.inf, -999, inplace=True)\n",
    "\n",
    "        #y_train.fillna(-999, inplace=True)\n",
    "        #y_train.replace(np.inf, -999, inplace=True)\n",
    "\n",
    "        train['Asset_ID'] = train['Asset_ID'].astype('category')\n",
    "\n",
    "        #ind_v = valid.index\n",
    "        #y_val = valid['Target'].values #* 1000\n",
    "        #valid = valid[features]\n",
    "        #valid.fillna(-999, inplace=True)\n",
    "        #valid.replace(np.inf, -999, inplace=True)\n",
    "        #valid['Asset_ID'] = valid['Asset_ID'].astype('category')\n",
    "\n",
    "        #weights_tr_fold = weights_tr[ind_t].reset_index(drop=True, inplace=True)\n",
    "        #weights_val_fold = weights_val[ind_v].reset_index(drop=True, inplace=True)\n",
    "        mem = psutil.virtual_memory() \n",
    "        print('Trainm前')\n",
    "        print(mem.percent)\n",
    "        print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "        print('Peak:', peak_memory()/10**6, 'GB')\n",
    "        print(\"=\"*100)\n",
    "\n",
    "\n",
    "        train_dataset = lgb.Dataset(train, y_train, feature_name = features, categorical_feature= ['Asset_ID'])\n",
    "        #val_dataset = lgb.Dataset(valid, y_val, feature_name = features, categorical_feature= ['Asset_ID'])\n",
    "        #train_dataset.add_w = weights_tr_fold\n",
    "        #val_dataset.add_w = weights_val_fold\n",
    "        train_dataset.add_w = weights_tr\n",
    "        #val_dataset.add_w = weights_val\n",
    "        #weights_train = train[['weights']]\n",
    "        #weights_test = test[['weights']]\n",
    "\n",
    "        #scaler = StandardScaler()\n",
    "        #scaler.fit(x_train)\n",
    "\n",
    "        #x_train = scaler.transform(x_train)\n",
    "        #print(x_train.shape)\n",
    "        print('A')\n",
    "        # メモリ使用率を取得\n",
    "        mem = psutil.virtual_memory() \n",
    "        print('Trainm前')\n",
    "        print(mem.percent)\n",
    "        print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "        print('Peak:', peak_memory()/10**6, 'GB')\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',  #objectives = ['regression','regression_l1', 'huber', 'fair','quantile', 'mape', 'gamma','tweedie']\n",
    "            #'fair_c': 100,\n",
    "            #'metric': 'None',\n",
    "            'boosting_type': 'gbdt',\n",
    "            #'max_bin':255,\n",
    "            'min_data_in_leaf':150,\n",
    "            'subsample': 0.7,\n",
    "            'subsample_freq': 5, # 何回に一回バギングするか\n",
    "            #'feature_fraction': 0.4,\n",
    "            'max_depth': -1,\n",
    "            #'num_leaves':32, \n",
    "            'learning_rate': 0.01,\n",
    "            #'subsample_freq': 4,\n",
    "            #'feature_fraction': 0.4,\n",
    "            'colsample_bytree': 0.9, \n",
    "            'lambda_l1': 10,\n",
    "            'lambda_l2': 10,\n",
    "            'seed': 1991,\n",
    "            'verbose': -1,\n",
    "            'n_jobs':-1\n",
    "            }\n",
    "\n",
    "        model = lgb.train(params = params,\n",
    "                              num_boost_round = 350, \n",
    "                              train_set = train_dataset, \n",
    "                              #valid_sets = [val_dataset],\n",
    "                              #early_stopping_rounds=20,\n",
    "                              verbose_eval = 50,\n",
    "                              feval=eval_wcorr,\n",
    "                              evals_result = evals_result \n",
    "                             )\n",
    "\n",
    "        #model = Ridge(alpha=5)\n",
    "        #model.fit(x_train, y_train)\n",
    "\n",
    "        #x_val = scaler.transform(x_val)\n",
    "        #val_pred = model.predict(valid)\n",
    "        #valid_oof[ind_v] = val_pred\n",
    "        #score = weighted_correlation(val_pred, y_val, weights_val)\n",
    "        models.append(model)\n",
    "        #print(\"Fold metric score:\", score)\n",
    "        #print(\"Fold single corr:\", np.corrcoef(val_pred, y_val)[0][1])\n",
    "        #print(pearsonr(y_val, val_pred))\n",
    "        print(\"=\"*100)\n",
    "\n",
    "        del train_dataset\n",
    "        #del val_dataset\n",
    "        gc.collect()\n",
    "\n",
    "        train_dataset = lgb.Dataset(train, y_train, feature_name = features, categorical_feature= ['Asset_ID'])\n",
    "        #val_dataset = lgb.Dataset(valid, y_val, feature_name = features, categorical_feature= ['Asset_ID'])\n",
    "        train_dataset.add_w = weights_tr\n",
    "        #val_dataset.add_w = weights_val\n",
    "\n",
    "        params2 = {\n",
    "            'objective': 'regression',  #objectives = ['regression','regression_l1', 'huber', 'fair','quantile', 'mape', 'gamma','tweedie']\n",
    "            #'fair_c': 100,\n",
    "            #'metric': 'None',\n",
    "            'boosting_type': 'gbdt',\n",
    "            #'max_bin':255,\n",
    "            'min_data_in_leaf':150,\n",
    "            'subsample': 0.7,\n",
    "            'subsample_freq': 5, # 何回に一回バギングするか\n",
    "            #'feature_fraction': 0.4,\n",
    "            'max_depth': -1,\n",
    "            #'num_leaves':32, \n",
    "            'learning_rate': 0.01,\n",
    "            #'subsample_freq': 4,\n",
    "            #'feature_fraction': 0.4,\n",
    "            'colsample_bytree': 0.9, \n",
    "            'lambda_l1': 10,\n",
    "            'lambda_l2': 10,\n",
    "            'seed': 91,\n",
    "            'verbose': -1,\n",
    "            'n_jobs':-1\n",
    "            }\n",
    "\n",
    "        model = lgb.train(params = params2,\n",
    "                              num_boost_round = 350, \n",
    "                              train_set = train_dataset, \n",
    "                              #valid_sets = [val_dataset],\n",
    "                              #early_stopping_rounds=20,\n",
    "                              verbose_eval = 50,\n",
    "                              feval=eval_wcorr,\n",
    "                              evals_result = evals_result \n",
    "                             )\n",
    "\n",
    "        #model = Ridge(alpha=5)\n",
    "        #model.fit(x_train, y_train)\n",
    "\n",
    "        #x_val = scaler.transform(x_val)\n",
    "        #val_pred2 = model.predict(valid)\n",
    "        #valid_oof2[ind_v] = val_pred2\n",
    "        #score = weighted_correlation(val_pred2, y_val, weights_val)\n",
    "        models2.append(model)\n",
    "        #print(\"Fold metric score:\", score)\n",
    "        #print(\"Fold single corr:\", np.corrcoef(val_pred2, y_val)[0][1])\n",
    "        #print(pearsonr(y_val, val_pred2))\n",
    "        print(\"=\"*100)\n",
    "\n",
    "\n",
    "        del train_dataset\n",
    "        #del val_dataset\n",
    "        gc.collect()\n",
    "\n",
    "        train_dataset = lgb.Dataset(train, y_train, feature_name = features, categorical_feature= ['Asset_ID'])\n",
    "        #val_dataset = lgb.Dataset(valid, y_val, feature_name = features, categorical_feature= ['Asset_ID'])\n",
    "        train_dataset.add_w = weights_tr\n",
    "        #val_dataset.add_w = weights_val\n",
    "\n",
    "        params3 = {\n",
    "            'objective': 'regression',  #objectives = ['regression','regression_l1', 'huber', 'fair','quantile', 'mape', 'gamma','tweedie']\n",
    "            #'fair_c': 100,\n",
    "            #'metric': 'None',\n",
    "            'boosting_type': 'gbdt',\n",
    "            #'max_bin':255,\n",
    "            'min_data_in_leaf':150,\n",
    "            'subsample': 0.7,\n",
    "            'subsample_freq': 5, # 何回に一回バギングするか\n",
    "            #'feature_fraction': 0.4,\n",
    "            'max_depth': -1,\n",
    "            #'num_leaves':32, \n",
    "            'learning_rate': 0.01,\n",
    "            #'subsample_freq': 4,\n",
    "            #'feature_fraction': 0.4,\n",
    "            'colsample_bytree': 0.9, \n",
    "            'lambda_l1': 10,\n",
    "            'lambda_l2': 10,\n",
    "            'seed': 31,\n",
    "            'verbose': -1,\n",
    "            'n_jobs':-1\n",
    "            }\n",
    "\n",
    "        model = lgb.train(params = params3,\n",
    "                              num_boost_round = 350, #int(300*1.1), \n",
    "                              train_set = train_dataset, \n",
    "                              #valid_sets = [val_dataset],\n",
    "                              #early_stopping_rounds=20,\n",
    "                              verbose_eval = 50,\n",
    "                              feval=eval_wcorr,\n",
    "                              evals_result = evals_result \n",
    "                             )\n",
    "\n",
    "        #model = Ridge(alpha=5)\n",
    "        #model.fit(x_train, y_train)\n",
    "\n",
    "        #x_val = scaler.transform(x_val)\n",
    "        #val_pred3 = model.predict(valid)\n",
    "        #valid_oof3[ind_v] = val_pred3\n",
    "        #score = weighted_correlation(val_pred3, y_val, weights_val)\n",
    "        models3.append(model)\n",
    "        #print(\"Fold metric score:\", score)\n",
    "        #print(\"Fold single corr:\", np.corrcoef(val_pred3, y_val)[0][1])\n",
    "        #print(pearsonr(y_val, val_pred3))\n",
    "        print(\"=\"*100)\n",
    "\n",
    "        del train_dataset\n",
    "        #del val_dataset\n",
    "        gc.collect()\n",
    "\n",
    "        ################################# ridge #################################\n",
    "        '''\n",
    "        model4 = Ridge(alpha=10)\n",
    "        # model = lgb.LGBMRegressor(n_jobs=-1, random_state=1)\n",
    "\n",
    "        model4 = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model4)\n",
    "        ])\n",
    "\n",
    "        model4.fit(train[features_ridge].values, y_train)\n",
    "\n",
    "        #model = Ridge(alpha=5)\n",
    "        #model.fit(x_train, y_train)\n",
    "\n",
    "        #x_val = scaler.transform(x_val)\n",
    "        #val_pred4 = model4.predict(valid[features_ridge].values)\n",
    "        #valid_oof[ind_v] = val_pred\n",
    "        #score = weighted_correlation(val_pred4, y_val, weights_val)\n",
    "        models4.append(model4)\n",
    "        #print(\"Fold metric score:\", score)\n",
    "        #print(\"Fold single corr:\", np.corrcoef(val_pred4, y_val)[0][1])\n",
    "        #print(pearsonr(y_val, val_pred4))\n",
    "        #print(\"=\"*100) \n",
    "        '''\n",
    "        for i in range(14):\n",
    "            model4 = Ridge(alpha=10)\n",
    "            # model = lgb.LGBMRegressor(n_jobs=-1, random_state=1\n",
    "            print('ASSET : ', i)\n",
    "            model4 = Pipeline([\n",
    "                ('scaler', StandardScaler()), \n",
    "                ('model', model4)\n",
    "            ])\n",
    "            indt = train[train['Asset_ID' ] == i].index\n",
    "            #indv = valid[valid['Asset_ID' ] == i].index\n",
    "            model4.fit(train.iloc[indt][features_ridge].values, y_train[indt])\n",
    "            #val_pred4 = model4.predict(valid.iloc[indv][features_ridge].values)\n",
    "            #valid_oof[ind_v] = val_pred\n",
    "            #score = weighted_correlation(val_pred4, y_val[indv], weights_val[indv])\n",
    "            models4.append(model4)\n",
    "            #valid_oofridge[indv] = val_pred4\n",
    "            #print(\"Fold metric score:\", score)\n",
    "            #print(\"Fold single corr:\", np.corrcoef(val_pred4, y_val[indv])[0][1])\n",
    "            #print(pearsonr(y_val[indv], val_pred4))\n",
    "            #print('='  * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "defb22d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:40:39.946176Z",
     "iopub.status.busy": "2022-01-31T17:40:39.945432Z",
     "iopub.status.idle": "2022-01-31T17:40:39.951683Z",
     "shell.execute_reply": "2022-01-31T17:40:39.952267Z",
     "shell.execute_reply.started": "2022-01-31T15:15:31.561334Z"
    },
    "papermill": {
     "duration": 2.67477,
     "end_time": "2022-01-31T17:40:39.952455",
     "exception": false,
     "start_time": "2022-01-31T17:40:37.277685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train後\n",
      "52.7\n",
      "Use Memory total: 9.264337299346924 GiB\n",
      "Peak: 14.792868 GB\n"
     ]
    }
   ],
   "source": [
    "# メモリ使用率を取得\n",
    "mem = psutil.virtual_memory() \n",
    "print('Train後')\n",
    "print(mem.percent)\n",
    "print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "print('Peak:', peak_memory()/10**6, 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d91db6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:40:45.199324Z",
     "iopub.status.busy": "2022-01-31T17:40:45.198611Z",
     "iopub.status.idle": "2022-01-31T17:40:45.203813Z",
     "shell.execute_reply": "2022-01-31T17:40:45.204699Z",
     "shell.execute_reply.started": "2022-01-31T15:15:31.574608Z"
    },
    "papermill": {
     "duration": 2.580859,
     "end_time": "2022-01-31T17:40:45.204935",
     "exception": false,
     "start_time": "2022-01-31T17:40:42.624076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.7\n",
      "Use Memory total: 9.264337299346924 GiB\n",
      "Peak: 14.792868 GB\n"
     ]
    }
   ],
   "source": [
    "# メモリ使用率を取得\n",
    "mem = psutil.virtual_memory() \n",
    "print(mem.percent)\n",
    "print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "print('Peak:', peak_memory()/10**6, 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "019763e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:40:50.385859Z",
     "iopub.status.busy": "2022-01-31T17:40:50.384779Z",
     "iopub.status.idle": "2022-01-31T17:40:50.392258Z",
     "shell.execute_reply": "2022-01-31T17:40:50.392815Z",
     "shell.execute_reply.started": "2022-01-31T15:15:31.590712Z"
    },
    "papermill": {
     "duration": 2.634573,
     "end_time": "2022-01-31T17:40:50.393040",
     "exception": false,
     "start_time": "2022-01-31T17:40:47.758467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.7\n",
      "Use Memory total: 9.264337299346924 GiB\n",
      "Peak: 14.792868 GB\n"
     ]
    }
   ],
   "source": [
    "# メモリ使用率を取得\n",
    "mem = psutil.virtual_memory() \n",
    "print(mem.percent)\n",
    "print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "print('Peak:', peak_memory()/10**6, 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b45309f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:40:55.610912Z",
     "iopub.status.busy": "2022-01-31T17:40:55.609770Z",
     "iopub.status.idle": "2022-01-31T17:40:55.806973Z",
     "shell.execute_reply": "2022-01-31T17:40:55.805639Z",
     "shell.execute_reply.started": "2022-01-31T15:15:31.608028Z"
    },
    "papermill": {
     "duration": 2.84689,
     "end_time": "2022-01-31T17:40:55.807139",
     "exception": false,
     "start_time": "2022-01-31T17:40:52.960249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del train\n",
    "gc.collect()\n",
    "\n",
    "if VALID:\n",
    "    del valid\n",
    "    gc.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "114991ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:41:00.965351Z",
     "iopub.status.busy": "2022-01-31T17:41:00.964450Z",
     "iopub.status.idle": "2022-01-31T17:41:00.967731Z",
     "shell.execute_reply": "2022-01-31T17:41:00.968293Z",
     "shell.execute_reply.started": "2022-01-31T15:15:31.763531Z"
    },
    "papermill": {
     "duration": 2.587342,
     "end_time": "2022-01-31T17:41:00.968465",
     "exception": false,
     "start_time": "2022-01-31T17:40:58.381123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asset_ID', 'HperL', 'log_r', 'log_ret', 'realized_vol_15', 'realized_vol_240', 'log_ret_15_lag', 'log_ret_15_std', 'ENV_up', 'ENV_do', 'RSI15', 'RSI240', 'macd60', 'signal15', 'signal60', 'realized_vol_std_sum_15', 'realized_vol_std_sum_240', 'm15_15mean', 'm15_240mean', 'm15_15std', 'vol240std', 'c240sum', 'log_ret_lag1', 'realized_vol_15_lag1', 'realized_vol_240_lag1', 'realized_vol_std_sum_15_lag1', 'realized_vol_std_sum_240_lag1', 'm15_15mean_lag1', 'm15_240mean_lag1', 'm15_15std_lag1', 'vol240std_lag1', 'c240sum_lag1', 'log_ret_lag2', 'realized_vol_15_lag2', 'realized_vol_240_lag2', 'realized_vol_std_sum_15_lag2', 'realized_vol_std_sum_240_lag2', 'm15_15mean_lag2', 'm15_240mean_lag2', 'm15_15std_lag2', 'vol240std_lag2']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(features)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10305902",
   "metadata": {
    "id": "a971b572",
    "papermill": {
     "duration": 2.671587,
     "end_time": "2022-01-31T17:41:06.218140",
     "exception": false,
     "start_time": "2022-01-31T17:41:03.546553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c54c645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:41:11.345480Z",
     "iopub.status.busy": "2022-01-31T17:41:11.344478Z",
     "iopub.status.idle": "2022-01-31T17:41:11.351455Z",
     "shell.execute_reply": "2022-01-31T17:41:11.352035Z",
     "shell.execute_reply.started": "2022-01-31T15:15:31.783382Z"
    },
    "papermill": {
     "duration": 2.559335,
     "end_time": "2022-01-31T17:41:11.352234",
     "exception": false,
     "start_time": "2022-01-31T17:41:08.792899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test前\n",
      "36.9\n",
      "Use Memory total: 6.486794048309326 GiB\n",
      "Peak: 14.792868 GB\n"
     ]
    }
   ],
   "source": [
    "# メモリ使用率を取得\n",
    "mem = psutil.virtual_memory() \n",
    "print('Test前')\n",
    "print(mem.percent)\n",
    "print(f'Use Memory total: {mem_bytes * (mem.percent / 100) / (2**30)} GiB')\n",
    "print('Peak:', peak_memory()/10**6, 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9ccbcdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:41:16.914574Z",
     "iopub.status.busy": "2022-01-31T17:41:16.913769Z",
     "iopub.status.idle": "2022-01-31T17:41:16.919834Z",
     "shell.execute_reply": "2022-01-31T17:41:16.920395Z",
     "shell.execute_reply.started": "2022-01-31T15:15:31.799544Z"
    },
    "papermill": {
     "duration": 2.71696,
     "end_time": "2022-01-31T17:41:16.920575",
     "exception": false,
     "start_time": "2022-01-31T17:41:14.203615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "ensweight = np.array([1,1,2,1,1,2,2,2,1,2,1,2,1,1])\n",
    "print(len(ensweight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4303cdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:41:22.190762Z",
     "iopub.status.busy": "2022-01-31T17:41:22.184860Z",
     "iopub.status.idle": "2022-01-31T17:41:22.288906Z",
     "shell.execute_reply": "2022-01-31T17:41:22.290045Z",
     "shell.execute_reply.started": "2022-01-31T15:15:31.80957Z"
    },
    "id": "c121b468",
    "papermill": {
     "duration": 2.716317,
     "end_time": "2022-01-31T17:41:22.290272",
     "exception": false,
     "start_time": "2022-01-31T17:41:19.573955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "CPU times: user 214 ms, sys: 1.96 ms, total: 216 ms\n",
      "Wall time: 93.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#not building the weights each loops\n",
    "asset_details = pd.read_csv('../input/g-research-crypto-forecasting/asset_details.csv')\n",
    "dict_weights = {}\n",
    "\n",
    "for i in range(asset_details.shape[0]):\n",
    "    dict_weights[asset_details.iloc[i,0]] = asset_details.iloc[i,1]\n",
    "weigths = np.array([dict_weights[i] for i in range(14)])\n",
    "\n",
    "# only needed when saving ?\n",
    "dtype={'Asset_ID': 'int8', 'Count': 'int32', 'row_id': 'int32', 'Count': 'int32',\n",
    "       'Open': 'float32', 'High': 'float32', 'Low': 'float32', 'Close': 'float32',\n",
    "       'Volume': 'float32', 'VWAP': 'float32'}\n",
    "#test_df = test_df.astype(dtype)\n",
    "\n",
    "#refactoring functions:\n",
    "\n",
    "# #instantiation Moving average features dict\n",
    "# dict_RM = {}\n",
    "# dict_RM_M = {}\n",
    "\n",
    "# for lag in MA_lags:\n",
    "#     dict_RM[lag] = RunningMean(lag)\n",
    "#     dict_RM_M[lag] = RunningMean(lag)\n",
    "    \n",
    "\n",
    "\n",
    "# #instantiation dict betas\n",
    "# dict_MM = {}\n",
    "# dict_Mr = {}\n",
    "\n",
    "#dict_RM = pickle.load(open('../input/on-line-feature-engineering/dict_RM_4.pkl', 'rb'))\n",
    "#dict_RM_M = pickle.load(open('../input/on-line-feature-engineering/dict_RM_M_4.pkl', 'rb'))\n",
    "#dict_MM = pickle.load(open('../input/on-line-feature-engineering/dict_MM_4.pkl', 'rb'))\n",
    "#dict_Mr = pickle.load(open('../input/on-line-feature-engineering/dict_MR_4.pkl', 'rb'))\n",
    "\n",
    "\n",
    "# for lag in beta_lags:\n",
    "#     dict_MM[lag] = RunningMean(lag)\n",
    "#     dict_Mr[lag] = RunningMean(lag)\n",
    "\n",
    "\n",
    "env = gresearch_crypto.make_env()\n",
    "iter_test = env.iter_test()\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    #print('A')\n",
    "    try:\n",
    "        test_df = test_df.fillna(0)\n",
    "\n",
    "        timestamp,Asset_ID,Count,O,H,L,C,Volume,VWAP,row_id = Clean_df(test_df.values)\n",
    "        #print('B')\n",
    "        # np.array([Count,O,H,L,C,Price,Volume,VWAP,Dollars,Volume_per_trade,Dollars_per_trade,log_ret,GK_vol,RS_vol])\n",
    "        Features = Base_Feature_fn(timestamp,Asset_ID,Count,O,H,L,C,Volume,VWAP)\n",
    "        baseinfo_features = np.delete(Features, np.s_[0:6], axis=1)\n",
    "        Features = np.delete(Features, np.s_[-8:], axis=1) \n",
    "        #print('C')   \n",
    "\n",
    "        #removing wieghts when data is missing so that they don't appears in market\n",
    "        weigthss = np.where(np.isnan(O),O,weigths) # ここ変えた\n",
    "        #Market_Features = np.nansum(Features*np.expand_dims(weigths,axis=1)/np.nansum(weigths),axis=0)\n",
    "        #Market_Features = np.tile(Market_Features,(14,1))\n",
    "\n",
    "        #np.array((sin_month,cos_month,sin_day,cos_day,sin_hour,cos_hour,sin_minute,cos_minute))\n",
    "        time = timestamp_to_date(timestamp[0])\n",
    "        Time_Features = Time_Feature_fn(time)\n",
    "        #Time_Features = np.tile(Time_Features,(14,1))\n",
    "\n",
    "        MA_Features = []\n",
    "        #MA_Features_M  = []\n",
    "        #MA_Features_max = [] # max featur\n",
    "        #MA_Features_min = [] # min feature\n",
    "        MA_Features_maxmin = []\n",
    "        RealV_Feature = []\n",
    "        RealV_Feature_15240 = []\n",
    "        last_log_r_Feature = []\n",
    "        Std_Features = []\n",
    "        Richman_Features = []\n",
    "        Richman_Features_beata = []\n",
    "        Btc_eth_corr_Features = []\n",
    "        Updo_rate_Features = []\n",
    "        MACD_Featuees = []\n",
    "        Market_Features = []\n",
    "        Volume_Features = []\n",
    "        Other_teq_Features = []\n",
    "        teq_lagfeatures = []\n",
    "        RSI_features = []\n",
    "\n",
    "        for lag in MA_lags:\n",
    "            dict_RM[lag].push(Features)\n",
    "            #dict_RM_M[lag].push(Market_Features)\n",
    "            dict_RM_base[lag].push(np.concatenate((baseinfo_features, weigthss.reshape(14,1)), axis=1)) # ここ変えた\n",
    "            #dict_RM_richbeta\n",
    "            #MA_Features.append(dict_RM[lag].get_mean())\n",
    "            #MA_Features_M.append(dict_RM_M[lag].get_mean())\n",
    "            #RealV_Feature.append(dict_RM[lag].get_realvol()) \n",
    "\n",
    "\n",
    "        last_log_r_Feature.append(dict_RM_base[15].get_lag()) # ここ\n",
    "        Std_Features.append(dict_RM[15].get_std())\n",
    "\n",
    "        beta_calc_features, rich_features = dict_RM_base[15].rich_cldiff() # richman feature1　ここ\n",
    "        Richman_Features.append(rich_features)\n",
    "\n",
    "\n",
    "        dict_RM_17[17].push(baseinfo_features[:,0].reshape(14,1))\n",
    "        rich_tech_fe = dict_RM_17[17].rich_beta()\n",
    "        Richman_Features_beata.append(rich_tech_fe)\n",
    "        #print('A')\n",
    "\n",
    "        dict_RM_richbeta[3601].push(baseinfo_features[:,0].reshape(14,1)) # richman feature2\n",
    "        macd_fe, rsi_fe = dict_RM_richbeta[3601].make_rsi()\n",
    "        RSI_features.append(rsi_fe)\n",
    "        #print('A')\n",
    "        #rich_tech_fe, macd_fe = dict_RM_richbeta[3601].rich_beta()\n",
    "\n",
    "\n",
    "\n",
    "        #print(Richman_Features_beata)\n",
    "\n",
    "        dict_RM_macd[10].push(macd_fe)\n",
    "        #print('A')\n",
    "        MACD_Featuees.append(np.concatenate((np.delete(macd_fe, np.s_[0], axis=1), dict_RM_macd[10].macd()), axis=1))\n",
    "        #print('A')\n",
    "\n",
    "        #Btc_eth_corr_Features.append(dict_RM_base[15].corr_btc_eth())\n",
    "        #Updo_rate_Features.append(dict_RM_base[15].updown_rate())\n",
    "\n",
    "        RealV_Feature.append(dict_RM[240].get_realvol())  # vola\n",
    "        #print('A')\n",
    "\n",
    "        dict_RM_vols[240].push(dict_RM[15].get_realvol())\n",
    "        RealV_Feature_15240.append(dict_RM_vols[240].vol_sum_std())\n",
    "\n",
    "        market_mean = np.nanmean(Features[:,5])\n",
    "        market_mean_abs = np.nanmean(abs(Features[:,5]))\n",
    "        dict_RM_market[240].push(np.array([market_mean, market_mean_abs]))\n",
    "        Market_Features.append(dict_RM_market[240].mmean_std())\n",
    "\n",
    "        Volume_Features.append(dict_RM_base[240].vol_features())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Other_teq_Features.append(dict_RM_base[240].oth_teq())\n",
    "        #print('zzz')\n",
    "        #RealV_Feature.append(dict_RM[60].get_realvol())\n",
    "\n",
    "        #for term in [60, 240, 1440]:\n",
    "        #  MA_Features_maxmin.append(dict_RM_base[term].get_maxmin())# maxmin\n",
    "          #print('A')\n",
    "\n",
    "\n",
    "        #print('A')\n",
    "        #MA_Features = np.concatenate(MA_Features,axis=1)\n",
    "        #print('B')\n",
    "        #MA_Features_M = np.concatenate(MA_Features_M)\n",
    "        #MA_Features_maxmin = np.concatenate(MA_Features_maxmin,axis=1) # maxmin\n",
    "        #print('C')\n",
    "\n",
    "        #MA_Features_max = np.concatenate(MA_Features_max,axis=1)\n",
    "        #MA_Features_min = np.concatenate(MA_Features_min,axis=1)\n",
    "        RealV_Feature = np.concatenate(RealV_Feature,axis=1) # vola\n",
    "\n",
    "        last_log_r_Feature = np.concatenate(last_log_r_Feature,axis=1) # lag\n",
    "        Std_Features = np.concatenate(Std_Features,axis=1) # 標準偏差\n",
    "\n",
    "        #Richman_Features = np.concatenate(Richman_Features, axis=1) # richman feature1\n",
    "        Richman_Features_beata = np.concatenate(Richman_Features_beata, axis=1) # richman feature1\n",
    "        RSI_features = np.concatenate(RSI_features, axis=1)\n",
    "        MACD_Featuees = np.concatenate(MACD_Featuees, axis=1)\n",
    "\n",
    "        RealV_Feature_15240 = np.concatenate(RealV_Feature_15240, axis=1)\n",
    "\n",
    "        Market_Features = np.concatenate(Market_Features, axis=1)\n",
    "        Volume_Features = np.concatenate(Volume_Features, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        dict_RM_lag[5].push(np.concatenate((np.delete(Features, np.s_[:-1], axis=1),  RealV_Feature, RealV_Feature_15240, Market_Features, Volume_Features), axis=1))\n",
    "        #print('A')\n",
    "        teq_lagfeatures.append(dict_RM_lag[5].tec_lags())\n",
    "        teq_lagfeatures = np.concatenate(teq_lagfeatures, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        Features_ridge = Features[:, [3, 5]]\n",
    "        teq_lagfeatures_ridge = np.delete(teq_lagfeatures, np.s_[9], axis=1)\n",
    "\n",
    "        Features = np.delete(Features, np.s_[1:3], axis=1)\n",
    "        Features = np.delete(Features, 2, axis=1)\n",
    "        #print('C')\n",
    "        #betas = np.concatenate(betas,axis=1)\n",
    "        #betas = np.nan_to_num(betas, nan=0., posinf=0., neginf=0.) \n",
    "        #Features = np.delete(Features, 4, axis=1)\n",
    "        #values = np.concatenate((Features,np.tile(Market_Features,(14,1)),np.tile(Time_Features,(14,1)),MA_Features,np.tile(MA_Features_M,(14,1)),betas, MA_Features_max, MA_Features_min, RealV_Feature, np.expand_dims(Target,axis=1)),axis=1)\n",
    "        #values = np.concatenate((Features,RealV_Feature,betas,MA_Features, MA_Features_maxmin,np.expand_dims(Target,axis=1)),axis=1)\n",
    "\n",
    "        values = np.concatenate((np.expand_dims(Asset_ID,axis=1), Features,RealV_Feature,last_log_r_Feature, Std_Features, #Richman_Features,\n",
    "                                              Richman_Features_beata, RSI_features,\n",
    "                                              MACD_Featuees, RealV_Feature_15240, Market_Features, \n",
    "                                              Volume_Features, #Other_teq_Features, \n",
    "                                              teq_lagfeatures, \n",
    "                                              ),axis=1)#values = np.concatenate((Features,RealV_Feature,last_log_r_Feature, Std_Features, Richman_Features,Richman_Features_beata, MACD_Featuees, Btc_eth_corr_Features, Updo_rate_Features, np.expand_dims(Target,axis=1)),axis=1)\n",
    "\n",
    "        #values = np.delete(values, np.s_[44:], axis=1)\n",
    "        values[np.isnan(values)] = -999\n",
    "        values[np.isnan(values)] = -999\n",
    "        values[values == np.inf] = -999\n",
    "\n",
    "\n",
    "        values_ridge = np.concatenate((Features_ridge,RealV_Feature,last_log_r_Feature, #Std_Features, #Richman_Features,\n",
    "                                       Richman_Features_beata, #RSI_features,\n",
    "                                       #MACD_Featuees, \n",
    "                                       #RealV_Feature_15240, \n",
    "                                       Market_Features, \n",
    "                                       #Volume_Features, #Other_teq_Features, \n",
    "                                       teq_lagfeatures_ridge, \n",
    "                                       ),axis=1)#values = np.concatenate((Features,RealV_Feature,last_log_r_Feature, Std_Features, Richman_Features,Richman_Features_beata, MACD_Featuees, Btc_eth_corr_Features, Updo_rate_Features, np.expand_dims(Target,axis=1)),axis=1)\n",
    "\n",
    "        #values = np.delete(values, np.s_[44:], axis=1)\n",
    "        values_ridge[np.isnan(values_ridge)] = -999\n",
    "        values_ridge[np.isnan(values_ridge)] = -999\n",
    "        values_ridge[values_ridge == np.inf] = -999\n",
    "\n",
    "\n",
    "\n",
    "        # diff features\n",
    "        #basep = [1,2,3,4,5]\n",
    "        #llag = [[19,20,21,22,23], [24,25,26,27,28]]\n",
    "        #for col in tqdm(range(len(basep))):\n",
    "        #    for lagi in range(len(llag)):\n",
    "            #print(col, lagf)\n",
    "        #        values[:,basep[col]] - values[:,llag[lagi][col]]\n",
    "\n",
    "        preds1 = (models[0].predict(values))\n",
    "        preds2 = (models2[0].predict(values))\n",
    "        preds3 = (models3[0].predict(values))\n",
    "\n",
    "        preds = (preds1 + preds2 + preds3) / 3\n",
    "        \n",
    "        preds_ridge = []\n",
    "        preds_final = []\n",
    "        for i in range(14):\n",
    "            pre = models4[i].predict(values_ridge)\n",
    "            \n",
    "            if ensweight[i] == 1:\n",
    "                preds_final.append(preds[i])\n",
    "            \n",
    "            else:\n",
    "                preds_final.append(pre[i])\n",
    "            #preds_ridge.append(pre[i])\n",
    "        #preds_ridge = (models4[0].predict(values_ridge))\n",
    "        \n",
    "        \n",
    "        #for i in range(14):\n",
    "        #    if ensweight[i] == 1:\n",
    "        #        preds_final.append(preds[i])\n",
    "        #    else:\n",
    "        #        preds_final.append(preds_ridge[i])\n",
    "                \n",
    "        preds_final = np.array(preds_final)\n",
    "        #weightv = 0.4\n",
    "        #preds = (preds * weightv) + (1-weightv) * np.array(preds_ridge)\n",
    "        #preds = preds * weight + (1-weight) * preds_ridge\n",
    "        #preds = np.array([model.predict(values) for model in models])\n",
    "        #preds = np.median(np.expm1(preds), axis=0)\n",
    "        #print('I')\n",
    "        #preds = model_lgbm[0].predict(values)\n",
    "        #preds = model.predict(scaler.transform(values))\n",
    "        #preds = np.median(np.array([model_lgbm[str(i)+'-'+str(j)].predict(values, num_iteration = ES_it[str(i)+'-'+str(j)]) for i in range(5) for j in range(5)]),axis=0)\n",
    "\n",
    "        sample_prediction_df['Target'] = [preds_final[(row_id == rid)][0] for rid in sample_prediction_df.row_id.values]\n",
    "        env.predict(sample_prediction_df)\n",
    "    \n",
    "    except:\n",
    "        a = np.array([ 2.08572321e-05,  4.94754720e-06, -3.10617725e-05,  1.57531076e-07,\n",
    "                      -3.63379728e-06, -1.23053704e-05,  1.12755761e-05, -2.44581399e-05,\n",
    "                      8.72486649e-05,  1.34530341e-05, -5.19872381e-08, -1.15822545e-05,\n",
    "                      5.64495861e-07, -1.69252929e-05])\n",
    "\n",
    "        sample_prediction_df['Target'] = a\n",
    "        env.predict(sample_prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffe39335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:41:27.484766Z",
     "iopub.status.busy": "2022-01-31T17:41:27.484075Z",
     "iopub.status.idle": "2022-01-31T17:41:27.494135Z",
     "shell.execute_reply": "2022-01-31T17:41:27.494811Z",
     "shell.execute_reply.started": "2022-01-31T15:15:31.947909Z"
    },
    "papermill": {
     "duration": 2.606104,
     "end_time": "2022-01-31T17:41:27.495017",
     "exception": false,
     "start_time": "2022-01-31T17:41:24.888913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>-0.000347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43</td>\n",
       "      <td>-0.016758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44</td>\n",
       "      <td>0.002395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>-0.000127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>47</td>\n",
       "      <td>-0.008034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>48</td>\n",
       "      <td>-0.030751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>49</td>\n",
       "      <td>0.000619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50</td>\n",
       "      <td>-0.000324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>51</td>\n",
       "      <td>-0.008875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>52</td>\n",
       "      <td>-0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>53</td>\n",
       "      <td>-0.000149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>54</td>\n",
       "      <td>-0.000657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>55</td>\n",
       "      <td>-0.008497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    row_id    Target\n",
       "0       42 -0.000347\n",
       "1       43 -0.016758\n",
       "2       44  0.002395\n",
       "3       45  0.000074\n",
       "4       46 -0.000127\n",
       "5       47 -0.008034\n",
       "6       48 -0.030751\n",
       "7       49  0.000619\n",
       "8       50 -0.000324\n",
       "9       51 -0.008875\n",
       "10      52 -0.000091\n",
       "11      53 -0.000149\n",
       "12      54 -0.000657\n",
       "13      55 -0.008497"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7870cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-31T17:41:32.686141Z",
     "iopub.status.busy": "2022-01-31T17:41:32.685397Z",
     "iopub.status.idle": "2022-01-31T17:41:32.693973Z",
     "shell.execute_reply": "2022-01-31T17:41:32.694532Z",
     "shell.execute_reply.started": "2022-01-31T15:15:32.036678Z"
    },
    "id": "40ba687a",
    "papermill": {
     "duration": 2.562856,
     "end_time": "2022-01-31T17:41:32.694715",
     "exception": false,
     "start_time": "2022-01-31T17:41:30.131859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: 14.792868 GB\n"
     ]
    }
   ],
   "source": [
    "# peak memory\n",
    "import os\n",
    "import re\n",
    "\n",
    "def peak_memory():\n",
    "    pid = os.getpid()\n",
    "    with open(f'/proc/{pid}/status') as f:\n",
    "        for line in f:\n",
    "            if not line.startswith('VmHWM:'):\n",
    "                continue\n",
    "            return int(re.search('[0-9]+', line)[0])\n",
    "    raise ValueError('Not Found')\n",
    "\n",
    "def f():\n",
    "    a = [0] * 20000000\n",
    "\n",
    "print('before:', peak_memory()/10**6, 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c736b",
   "metadata": {
    "papermill": {
     "duration": 3.194849,
     "end_time": "2022-01-31T17:41:39.667689",
     "exception": false,
     "start_time": "2022-01-31T17:41:36.472840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1962.660291,
   "end_time": "2022-01-31T17:41:43.952550",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-31T17:09:01.292259",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
